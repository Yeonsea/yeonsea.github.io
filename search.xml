<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[软件架构设计原则与设计模式]]></title>
    <url>%2F2019%2F10%2F09%2F%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%B8%8E%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[软件架构设计原则开闭原则Open-Closed Principle，OCP是指一个软件实体(如类、模块和函数)应该对扩展开放，对修改关闭。 依赖倒置原则Dependence Inversion Principle，DIP是指设计代码结构时，高层模块不应该依赖低层模块，二者都应该依赖其抽象。抽象不应该依赖细节，细节应该依赖抽象。 单一职责原则Simple Responsibiy Principle，SRP是指不要存在多于一个导致类变更的原因。总体来说，就是一个类、接口或方法只负责一项职责。 接口隔离原则Interface Segregation Principle，ISP是指用多个专门的接口，而不使用单一的总接口，客户端不应该依赖它不需要的接口。 迪米特原则Law of Demeter，LoD是指一个对象应该对其他对象保持最少了解，又叫最少知道原则(Least Knowledge Principle，LKP)，尽量降低类与类之间的耦合度。 里氏替换原则Liskov Substitution Principle，LSP是指如果对每一个类型为 T1 的对象 o1，都有类型为 T2 的对象 o2，使得以 T1 定义的所有程序 P 在所有对象 o1 都替换成 o2 时，程序 P 的行为没有发生变化，那么类型 T2 是类型 T1 的子类型。 合成复用原则Composite/Aggregate Reuse Principle，CARP是指尽量使用对象组合(has-a)/聚合(contanis-a)而不是继承关系达到软件复用的目的。 Spring 中常用的设计模式设计模式作用： 写出优雅的代码 更好地重构项目 经典框架都在用设计模式解决问题：命名对照： 工厂模式 BeanFactory 装饰者模式 BeanWrapper 代理模式 AopProxy 委派模式 DispatcherServlet 策略模式 HandlerMapping 适配器模式 HandlerAdapter 模板模式 JdbcTemplate 观察者模式 ContextLoaderListener 设计模式从来都不是单个模式的独立使用。在实际应用中，通常是将多个设计模式混合使用 工厂模式简单工厂模式Simple Factory Pattern是指由一个工厂对象决定创建哪一种产品类的实例，但它不属于 GoF 的 23 种设计模式。 示例：JDK 的 Calendar#getInstancelogback 的 LoggerFactory#getLogger 总结：适用于工厂类负责创建的对象较少的场景；工厂类的职责相对过重，不易于扩展过于复杂的产品结构。 工厂方法模式Factory Method Pattern是指定义一个创建对象的接口，但让实现这个接口的类来决定实例化哪个类，工厂方法模式让类的实例化推迟到子类中进行。在工厂方法模式中用户只需要关心所需产品对应的工厂，无需关心创建细节，而且加入新的产品时符合开闭原则。 示例：logback ILoggerFactory 工厂方法模式适用一下场景： 创建对象需要大量重复的代码 客户端(应用层)不依赖于产品类实例如何被创建、如何被实现等细节。 一个类通过其子类来指定创建哪个对象 工厂方法模式的缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象工厂模式Abastract Factory Patten是指创建一系列相关或互相依赖对象的接口，无须指定他们的具体类。 产品族、产品系列 抽象工厂模式的缺点： 规定了所有可能被创建的产品集合，产品族中扩展新的产品困难，需要修改抽象工厂的接口 增加了系统的抽象性和理解难度 利用工厂模式重构的实践案例数据库连接池 单例模式详解单例模式的应用场景Singleton Pattern是指确保一个类在任何情况下都绝对只有一个实例，并提供一个全局访问点。 示例： J2EE 标准中的 ServletContext、ServletContextConfig等 Spring 框架应用中的 ApplicationContext 数据库的连接池 饿汉式单例模式饿汉式单例模式在类加载的时候就立即初始化，并且创建单例对象。它绝对线程安全，在线程还没出现以前就实例化了，不可能存在访问安全问题。 优点：没有加任何锁、执行效率比较高，用户体验比懒汉式单例模式更好。缺点：类加载的时候就初始化，不管用与不用都占着空间，浪费了内存。 Spring IoC 容器 ApplicationContext 适用于单例对象较少的情况 懒汉式单例模式被外部类调用的时候内部类才会加载。 双重加锁静态内部类 反射破坏单例静态内部类 构造方法做限制 序列化破坏单例反序列化后的对象会重新分配内存，即重新创建。ObjectInputStream#readObject 通过 JDK 源码分析可以看出，虽然增加 readResolve() 方法返回实例解决了单例模式被破坏的问题，但是实际上实例化了两次，只不过新创建的对象没有被返回而已。 注册式单例模式又称为登记式单例模式，就是将每一个实例都登记到某一个地方，使用唯一的标识获取实例。 枚举式单例模式枚举式单例模式在静态代码块中就给 INSTANCE 进行了赋值，是饿汉式单例模式的实现。 枚举类型其实通过类名和类对象类找到一个唯一的枚举对象。因此，枚举对象不可能被类加载器加载多次。 JDK 枚举的语法特殊性及反射也为枚举保驾护航，让枚举式单例模式成为一种比较优雅的实现 容器式单例容器式单例模式适用于实例非常多的情况，便于管理。但它是非线程安全的。 示例：spring 中容器式单例模式 线程单例实现 ThreadLocalThreadLocal 不能保证其创建的对象是全局唯一的，但是能保证在单个线程中是唯一的，天生是线程安全的。 ThreadLocal 将所有的对象全部放在 ThreadLocalMap 中，为每个线程都提供一个对象，实际上是以空间换时间来实现线程隔离的。 原型模式详解原型模式的应用场景Prototype Pattern是指原型实例指定创建对象的种类，并且通过这些原型创建新的对象。 适用场景： 类初始化消耗资源较多 使用 new 生成一个对象需要非常烦琐的过程(数据准备、访问权限等) 构造函数比较复杂 在循环体中产生大量对象 示例： spring scope=“prototype” JSON.parseObject() 浅克隆浅克隆复制的不是值，而是引用的地址。浅克隆只是完整复制了值类型数据，没有赋值引用对象。换言之，所有的引用对象仍然指向原来的对象。 深克隆实现 Cloneabl Serializable 接口 克隆破坏单例模式A: 要么单例类不实现 Cloneable 接口，要么重写 clone() 方法 实例：ArrayList#clone() 代理模式详解代理模式的应用场景Proxy Pattern是指为其他对象提供一种代理，以控制对这个对象的访问。 目的： 保护目标对象 增强目标对象 静态代理动态代理JDK 实现方式JDK 动态代理采用字节重组，重新生成对象来替代原始对象，以达到动态代理的目的。步骤如下： 获取被代理对象的引用，并且获取它的所有接口，反射获取 JDK 动态代理类重新生成一个新的类，同时新的类要实现被代理类实现的所有接口 动态生成 Java 代码，新加的业务逻辑方法由一定的逻辑代码调用(在代码中体现) 编译新生成的 Java 代码 .class 文件 重新加载到 JVM 中运行 以上过程就叫字节码重组。JDK 中有一个规范，在 ClassPath 下只要是 $ 开头的 .class 文件，一般都是自动生成的。 CGLib 代理调用 API CGLib 代理执行代理方法的效率之所以比 JDK 的高，是因为 CGlib 采用了 FastClass 机制，它的原理简单来说就是：为代理类和被代理类各生成一个类，这个类会为代理类或被代理类的方法分配一个 index(int 类型)，这个 index 当做一个入参，FastClass 就可以直接定位到要调用的方法并直接进行调用，省去了反射调用，所以调用效率比 JDK 代理通过反射调用高。 FastClass 并不是跟代理类一起生成的，而是在第一次执行 MethodProxy 的 invoke() 或 invokeSuper() 方法时生成的。 CGLib 和 JDK 动态代理对比 JDK 动态代理实现了被代理对象的接口，CGLib 代理继承了被代理对象 JDK 动态代理和 CGLib 代理都在运行期间生成字节码，JDK 动态代理直接写 Class 字节码，CGLib 代理使用 ASM 框架写 Class 字节码，CGlib 代理实现更复杂，生成代理类比 JDK 动态代理效率低。 JDK 动态代理调用代理方法是通过反射机制调用的，CGLib 代理是通过 FastClass 机制直接调用方法的，CGLib 执行的效率更高。 代理模式与 Spring代理模式在 Spring 源码中的应用Spring 利用动态代理实现 AOP 时有两个非常重要的类：JdkDynamicAopProxy 类和 CglibAopProxy 类。 Spring 中的代理选择原则 当 Bean 有实现接口时，Spring 就会用 JDK 动态代理 当 Bean 没有实现接口时，Spring 就会用 CGLib 代理 Spring 可以通过配置强制使用 CGLib 代理，需加入配置1&lt;aop:aspectj-autoproxy proxy-target-class="true" /&gt; 静态代理和动态代理的本质区别 静态代理只能通过手动完成代理操作，如果被代理类增加了新的方法，代理类需要同步增加，违背开闭原则。 动态代理采用在运行时动态生成代码的方式，取消了被代理类的扩展限制，遵循开闭原则。 若动态代理要对目标类的增强逻辑进行扩展，结合策略模式，只需要新增策略类便可完成，无语修改代理类的代码。 代理模式的优缺点优点： 代理模式能将代理对象与真实被调用目标对象分离。 在一定程度上降低了系统的耦合性，扩展性好。 可以起到保护目标对象的作用。 可以增强目标对象的功能。 缺点： 代理模式会造成系统设计中类的数量增加。 在客户端和目标对象中增加一个代理对象，会导致请求处理速度变慢。 增加了系统的复杂度。 委派模型详解委派模型定义及应用场景Delegate Pattern不属于 GoF 23 种设计模式基本作用就是负责任务的调用和分配，跟代理模式很像，可以看作一种特殊情况下的静态的全权代理，但是代理模式注重过程，而委派模式注重结果。 Spring#DispatcherServlet 示例：以 Delegate 结尾的地方都实现了委派模式 BeanDefinitionParserDelegate 根据不同类型委派不同的逻辑解析 BeanDefinition 策略模式详解Strategy Patten是指定义了算法家族并分别封装起来，让他们之间可以互相替换，此模式使得算法的变化不会影响使用算法的用户。 策略模式应用场景 系统中有很多类，而他们的区别仅仅在于行为不同 一个系统需要动态地在几种算法中选择一种 用策略模式实现选择支付方式的业务场景策略模式在 JDK 源码中的体现Comparator 接口的 compare 方法 Arrays类的 parallelSort 方法 TreeMap 类的构造方法 Spring Spring 中 Resource 接口 Spring 的初始化，InstantiationStrategy 策略模式的优缺点优点 策略模式符合开闭原则 策略模式可避免使用多重条件语句，如 if…else 语句、switch 语句 使用策略模式可以提高算法的保密性和安全性 缺点 客户端必须知道所有的策略，并且自行决定使用哪一个策略类 代码中会产生非常多的策略类，增加了代码的维护难度 委派模式与策略模式的综合应用DispatcherServlet 模板模式详解又叫模板方法模式Template Method Pattern是指定义一个算法的骨架，并允许子类为一个或者多个步骤提供实现。 模板模式的应用场景 一次性实现一个算法的不变部分，并将可变的行为留给子类来实现。 各子类中公共的行为被提取出来并集中到一个公共的父类中，从而避免代码重复。 设计钩子方法的主要目的是干预执行流程，使得控制行为更加灵活，更符合实际业务的需求。钩子方法的返回值一般为适合条件分支语句的返回值。 利用模板模式重构 JDBC 操作业务场景JdbcTemplate 模板模式在源码中的体现 AbstractList AbstractSet AbstractMap HttpServlet 的 service doGet doPost Mybatis 框架中的 BaseExecutor 类 模板模式的优缺点优点 利用模板模式将相同处理逻辑的代码放到抽象父类中，可以提高代码的复用性。 将不同的代码放到不同的子类中，通过对子类的扩展增加新的行为，可以提高代码的扩展性。 把不变的行为写在父类中，去除子类的重复代码，提供了一个很好的代码复用平台，符合开闭原则。 缺点 每个抽象类都需要一个子类来实现，导致了类的数量增加。 类数量的增加间接地增加了系统的复杂性。 因为继承关系自身的缺点，如果父类添加新的抽象方法，所有子类都要改一遍。 适配器模式详解Adapter Patten是指将一个类的接口转换成用户期望的另一个接口，使原本接口不兼容的类可以一起工作，属于结构型设计模式。 适配器模式的应用场景 已经存在的类的方法和需求不匹配(方法结果相同或相似)的情况。 适配器模式不是软件初始阶段考虑的设计模式，是随着软件的发展，由于不同产品、不同厂家造成功能类似而接口不同的问题的解决方案，有点亡羊补牢的感觉。 重构第三方登录自由适配的业务场景适配器模式在源码中的体现Spring AOP 中的 AdvisorAdapter 类有三个实现类： MethodBeforeAdviceAdapter AfterReturningAdviceAdapter ThrowsAdviceAdapter Spring MVC 中的 HandlerAdapter 类，调用的关键代码在 DispatcherServlet#doDispatch 适配器模式的优缺点优点： 能提高类的透明性和复用性，现有的类会被复用但不需要改变 目标类和适配器类解耦，可以提高程序的扩展性 在很多业务场景中符合开闭原则 缺点： 在适配器代码编写过程中需要进行全面考虑，可能会增加系统的复杂性 增加了代码的阅读难度，降低了代码的可读性，过多使用适配器会使系统的代码变得凌乱 装饰者模式详解Decorator Paftern是指在不改变原有对象的基础上，将功能附加到对象上，提供了比继承更有弹性的方案(扩展原有对象的功能)，属于结构型模式。 装饰者模式的应用场景 扩展一个类的功能或给一个类添加附加职责 动态给一个对象添加功能，这些功能可以再动态地撤销 装饰者模式最本质的特征是将原有类的附加功能抽离出来，简化原有类的逻辑。抽象的装饰者是可有可无的，具体可以根据业务模型来选择。 装饰者模式和适配器模式对比装饰者模式和适配器模式都是包装模式(Wrapper Pattern)，装饰者模式也是一种特殊的代理模式。 装饰者模式在源码中的应用 在 JDK 中体现最明显的类就是 IO 相关的类 Spring 中的 TransactionAwareCacheDecorator 类，这个类主要用来处理事务缓存，是对 Cache 的一个包装。 MyBatis 的 org.apache.ibatis.cache.Cache 类，比如 FifoCache 是先入先出算法的缓存，LruCache 是最近很少使用的缓存，TransactionalCache 是事务相关的缓存。 装饰者模式的优缺点优点： 装饰者模式是继承的有力补充，且比继承灵活，可以在不改变原有对象的情况下动态地给一个对象扩展功能，即插即用。 使用不同的装饰类及这些装饰类的排列组合，可以实现不同的效果。 装饰者模式完全符合开闭原则。 缺点： 会出现更多的代码、更多的类，增加程序的复杂性。 动态装饰时，多层装饰会更加复杂。 观察者模式详解Observe Patten定义了对象之间的一对多依赖，让多个观察者对象同时监听一个主体对象，当主体对象发生变化时，它的所有依赖者(观察者)都会收到通知并更新，属于行为型模式。 观察者模式的应用场景主要用于在关联行为之间建立一套触发机制的场景。 观察者模式在源码中的应用Spring 中的 ContextLoaderListener 类实现了 ServletContextListener 接口，ServletContextListener 接口又继承了 EventListener 接口 基于 Guava API 轻松落地观察者模式观察者模式的优缺点优点： 在观察者和被观察者之间建立了一个抽象的耦合 观察者模式支持广播通信 缺点： 观察者之间有过多的细节依赖、时间消耗多，程序的复杂性更高 使用不当会出现循环调用 各种设计模式的总结与对比设计模式其实是一门艺术。设计模式来源于生活，不要为了套用设计模式而使用设计模式。 设计模式是经验之谈，总结的是前人的经验，提供给后人借鉴使用。 设计模式可以帮助我们提升代码的可读性、可扩展性，降低维护成本，解决复杂的业务问题。]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向服务的体系架构]]></title>
    <url>%2F2019%2F09%2F24%2F%E9%9D%A2%E5%90%91%E6%9C%8D%E5%8A%A1%E7%9A%84%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[垂直应用架构解决了单一应用架构所面临的扩容问题，流量能够分散到各个子系统当中，且系统的体积可控，一定程度上降低了开发人员之间协同和维护的成本，提升了开发效率。 但是，当垂直应用越来越多，达到一定规模时，应用之间相互交互、相互调用便不可避免。否则，不同系统之间存在着重叠的业务，容易形成信息孤岛，重复造轮子。此时，相对核心的业务将会被抽取出来，作为单独的系统对外提供服务，达成业务之间相互复用，系统也由此演变为分布式应用架构体系。 分布式应用架构所面临的首要问题，便是如何实现应用之间的远程调用（RPC）。基于 HTTP 协议的系统间的 RPC，具有使用灵活、实现便捷（多种开源的 Web 服务器支持）、开放（国际标准）且天生支持异构平台之间的调用等多个优点，得到了广泛的使用。与之对应的是基于 TCP 协议实现的版本，它效率更高，但实现起来更加复杂，且由于协议和标准的不同，难以进行跨平台和企业间的便捷通信。当服务越来越多时，使得原本基于 F5、LVS 等负载均衡策略、服务地址管理和配置变得相当复杂和繁琐，单点的压力也变得越来越大。服务的动态注册和路由、更加高效的负载均衡的实现，成为了亟待解决的问题。 基于 TCP 协议的 RPCRPC 名称解释Remote Process Call，即远程过程调用，拥有 RMI、WebService 等诸多成熟的方案。 对象的序列化无论何种类型的数据，最终都需要转换成二进制流在网络上进行传输，在面向对象的设计中，数据的发送方需要将对象转换成为二进制流，才能在网络上进行传输，而数据的接收方则需要把二进制流再恢复为对象。 将对象转换为二进制流的过程称为对象的序列化。 将二进制流恢复为对象的过程称为对象的反序列化。 常用的解决方案： Google 的 Protocol Buffers 性能优异，跨平台，但编程代码侵入性较强，需要编写 proto 文件 Java 本身内置的序列化方式 Hessian JSON XML Java 内置的序列化方式所实现的对象序列化和反序列化关键代码：12345678910111213// 定义一个字节数组输出流ByteArrayOutputStream os = new ByteArrayOutputStream();// 对象输出流ObjectOutputStream out = new ObjectOutputStream(os);// 将对象写入到字节数组输出，进行序列化out.writeObject(yeon);byte[] yeonByte = os.toByteArray();// 字节数组输入流ByteArrayInputStream is = new ByteArrayInputStream(yeonByte);// 执行反序列化，从流中读取对象ObjectInputStream in = new ObjectInputStream(is);Person person = (Person) in.readObject(); 使用 Hessian 进行序列化，需要引入其提供的 hessian-4.0.7.jar12345678910ByteArrayOutputStream os = new ByteArrayOutputStream();// Hessian 的序列化输出HessianOutput ho = new HessianOutput(os);niho.writeObject(yeon);byte[] yeonByte = os.toByteArray();ByteArrayOutputStream os = new ByteArrayOutputStream();// Hessian 的反序列化读取对象HessianInput hi = new HessianInput(os);Person person = (Person) hi.readObject(); 基于 TCP 协议实现 RPC基于 Java 的 Socket API，能够实现一个简单 RPC 调用，包括了服务的接口及接口的远端实现、服务的消费者与远端的提供方。服务接口和实现：12345678910111213141516171819public interface SayHelloService &#123; /** * 问候的接口 * @param helloArg 参数 * @return */ public String sayHello(String helloArg);&#125;public class SayHelloServiceImpl implements SayHelloService &#123; @Override public String sayHello(String helloArg) &#123; if (helloArg.equals("hello")) &#123; return "hello"; &#125; else &#123; return "bye bye"; &#125; &#125;&#125; 服务消费者 Consumer 类的部分关键代码：123456789101112131415161718192021// 接口名称String interfacename = SayHelloService.class.getName();// 需要远程执行的方法Method method = SayHelloService.class.getMethod("sayHello", java.lang.String.class);// 需要传递到远端的参数Object[] arguments = &#123;"hello"&#125;;Socket socket = new Socket("127.0.0.1", 1234);// 将方法名和参数传递到远端ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream);output.writeUTF(interfacename); // 接口名称output.writeUTF(method.getName()); // 方法名称output.writeObject(method.getParameterTypes());output.writeObject(arguments);// 从远端读取方法执行结果ObjectInputStream input = new ObjectInputStream(socket.getInputStream());Object result = input.readObject(); 此处为阻塞IO，实际中应使用非阻塞IO，以提供更大的吞吐量 服务提供者 Provider 类的部分关键代码1234567891011121314151617181920ServerSocket server = new ServerSocket(1234);while(true) &#123; Socket socket = server.accept(); // 读取服务信息 ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); String interfacename = input.readUTF(); // 接口名称 String methodName = input.readUTF(); // 方法名称 Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[]) input.readObject(); // 参数类型 Object[] arguments = (Object[]) input.readObject(); // 参数对象 // 执行调用 Class serviceinterfaceclass = Class.forName(interfacename); // 得到接口的 Class Object service = services.get(interfacename); // 取得服务实现的对象 Method method = serviceinterfaceclass.getMethod(methodName, parameterTypes); // 获得要调用的方法 Object result = method.invoke(service, arguments); ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); output.writeObject(result);&#125; 服务提供端事先将服务实例化好后放在 services 这个 Map (涉及服务的路由被简化处理了)，通过一个 while 循环，不断地接收新到来的请求，得到所需要的参数，包括接口名称、方法名称、参数类型和参数，通过 Java 的反射取得接口中需要调用的方法，执行后将结果返回给服务的消费者。 基于 HTTP 协议的 RPCHTTP 协议栈HTTP 协议属于应用层协议，它不需要处理下层协议间诸如丢包补发、握手及数据的分段和重新组装等繁琐的细节，从而使开发人员可以专注于上层应用的设计。 基于 Java 的 Socket API 接口，设计一个简单的应用层通信协议：协议请求的定义：12345678910111213141516public class Request &#123; /** * 协议编码 */ private byte encode; /** * 命令 */ private Sting command; /** * 命令长度 */ private int commandLength;&#125; 协议响应的定义：12345678910111213141516public class Response &#123; /** * 编码 */ private byte encode; /** * 响应 */ private Sting response; /** * 响应长度 */ private int responseLength;&#125; 客户端实现：123456789101112131415// 请求Request request = new Request();request.setCommand("HELLO");request.setCommandLength(request.getCommand().length());request.setEncode(Encode.UTF8.getValue());Scoket client = new Socket("127.0.0.1", 4567);OutputStream output = client.getOutputStream();// 发送请求ProtocolUtil.writeRequest(output, request);// 读取响应数据InputStream input = client.getInputStream();Response response = ProtocolUtil.readResponse(input); 服务端实现：123456789101112131415161718192021ServerSocket server = new ServerSocket(4567);while(true) &#123; Socket client = server.accept(); // 读取响应数据 InputStream input = client.getInputStream(); Request request = ProtocolUtil.readRequest(input); OutputStream output = client.getOutputStream(); // 组装响应 Response response = new Response(); response.setEncode(Encode.UTF8.getValue()); if (request.getCommand().equals("HELLO")) &#123; response.setResponse("hello!"); &#125; else &#123; response.setResponse("bye bye!"); &#125; response.setResponseLength(response.getResponse().lenth()); ProtocolUtil.writeResponse(output, response);&#125; ProtocolUtil 的部分代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ProtocolUtil &#123; public static Request readRequest(InputStream input) throws IOException &#123; // 读取编码 byte[] encodeByte = new byte[1]; input.read(encodeByte); byte encode = encodeByte[0]; // 读取命令长度 byte[] commandLengthBytes = new byte[4]; input.read(commandLengthBytes); int commandLength = ByteUtil.bytes2Int(commandLengthBytes); // 读取命令 byte[] commandBytes = new byte[commandLength]; input.read(commandBytes); String command = ""; if (Encode.GBK.getValue() == encode) &#123; command = new String(commandBytes, "GBK"); &#125; else &#123; command = new String(commandBytes, "UTF8"); &#125; // 组装请求返回 Request request = new Request(); request.setCommand(command); request.setEncode(encode); request.setCommandLength(commandLength); return request; &#125; public static void writeResponse(OutputStream output, Response response) throws IOException &#123; // 将 response 响应返回给客户端 output.write(response.getEncode()); // output.write(response.getResponseLength()); // 直接 write 一个 int 类型会截取低 8 位传输，丢弃高 24 位 output.write(ByteUtil.int2ByteArray(response.getResponseLength())); if (Encode.GBK.getValue() == response.getEncode()) &#123; output.write(response.getResponse.getBytes("GBK")); &#125; else &#123; output.write(response.getResponse.getBytes("UTF8")); &#125; out.flush(); &#125;&#125; Big Endian 字节序转换：12345678910111213141516public static int byte2Int(byte[] bytes) &#123; int num = bytes[3] &amp; 0xFF; num |= ((bytes[2] &lt;&lt; 8) &amp; 0xFF00); num |= ((bytes[1] &lt;&lt; 16) &amp; 0xFF0000); num |= ((bytes[0] &lt;&lt; 24) &amp; 0xFF000000); return num;&#125;public static byte[] int2ByteArray(int i) &#123; byte[] result = new byte[4]; result[0] = (byte)((i &gt;&gt; 24) &amp; 0xFF); result[1] = (byte)((i &gt;&gt; 16) &amp; 0xFF); result[2] = (byte)((i &gt;&gt; 8) &amp; 0xFF); result[3] = (byte)(i &amp; 0xFF); return result;&#125; HTTP 请求与响应 首先，用户在浏览器地址栏输入 http://www.google.com/index.html 如果不指定端口，浏览器会默认为 80 端口。 操作系统通过查找 DNS 服务器进行域名解析，得到 www.google.com 对应的 IP 地址（74.125.31.147）。 浏览器发起并建立到 IP 地址的服务器 80 端口的连接，并向其发送 HTTP GET 请求。 服务端收到客户端发送的 HTTP GET 请求后，会响应一段 HTML 代码。浏览器根据接收到的 HTML 代码，下载相应的资源并进行网页的渲染。 通过 HTTPClient 发送 HTTP 请求 随着 HTTP 协议的广泛使用，它已经不仅仅局限于原来的 B/S 模式。很多情况下，我们需要自己实现向服务端发送请求，以及解析服务器响应这个过程。但是，像之前一样通过 Socket API 来实现这一切，会带来相当大的工作量，并且这种工作是重复且没有价值的，还有可能导致一些不可预估的风险，比如底层的流处理、并发控制等。HttpClient 的出现，为这一系列问题提供了一个成熟的解决方案。 HTTPClient 是 Apache 下的一个子项目，它对 HTTP 协议通信的过程进行了封装，提供高效且功能丰富的客户端编程工具包。示例：12345678910111213// url 前面请加上 HTTP 协议头，标名该请求为 HTTP 请求String url = "http://www.google.com";// 组装请求HttpClient httpClient = new DefaultHttpClient();HttpGet httpGet = new HttpGet(url);// 接收响应HttpResponse response = httpClient.execute(httpGet);HttpEntity entity = response.getEntity();byte[] bytes = EntityUtils.toByteArray(entity);String result = new String(bytes, "utf8"); 使用 HTTP 协议的优势 跨平台 不用关注底层 JSON 和 XML将 Java 对象 person 序列化成为 JSON 对象：使用 jackson-all-1.7.6.jar 工具包123456789101112131415161718192021222324252627282930Person person = new Person();person.setAddress("shanghai, China");person.setAge(18);person.setBirth(new Date());person.setName("yeon");// JSON 对象序列化String personJson = null;ObjectMapper mapper = new ObjectMapper();StringWriter sw = new StringWriter();JsonGenerator gen = new JsonFactory().createJsonGenerator(sw);mapper.writeValue(gen, person);gen.close();personJson = sw.toString();// JSON 对象反序列化Person yeon = (Person) mapper.readValue(personJson, Person.class);``` 将 Java 对象序列化成 XML 格式：使用 xstream-1.4.4.jar 工具包```java// 将 Person 对象序列化为 XMLXStream xStream = new xStream(new DomDriver());// 设置 Person 类的别名xStream.alias("person", Person.class);String personXML = xStream.toXML(person);// 将 XML 反序列化还原为 persion 对象Person yeon = (Person) xStream.fromXML(personXML); RESTful 和 RPCSpringMVC 来实现 RESTful 风格的 URL： 相对来说，RPC 风格的 URL 更接近于传统的设计模式，更容易被开发者所接受和理解，同时也更方便传统应用的接入和使用。而 RESTful 风格的本意是用 HTTP 协议所定义的几种操作方式来代替 RPC 中通过参数传输的操作类型，URL 看起来更加简洁，抽象程度更高，对业务扩展和复用有利，但是理解和实现起来更加复杂，适合简单的数据类服务，而非复杂的非资源操作类服务。 基于 HTTP 协议的 RPC 实现服务路由和负载均衡服务化的演变 分布式应用架构体系对于业务逻辑复用的需求十分强烈，上层业务都想借用已有的底层服务，来快速搭建更多、更丰富的应用，降低新业务开展的人力和时间成本，快速满足瞬息万变的市场需求。公共的业务被拆分出来，形成可共用的服务，最大程度地保障了代码和逻辑的复用，避免重复建设，这种设计也称为 SOA（Service-Oriented Architecture）。 SOA 架构中，服务消费者通过服务名称，在众多服务中找到要调用的服务的地址列表，称为服务的路由。 对于负载较高的服务来说，往往对应着由多台服务器组成的集群。在请求到来时，为了将请求均衡地分配到后端服务器，负载均衡程序将从服务对应的地址列表中，通过相应的负载均衡算法和规则，选取一台服务器进行访问，这个过程称为服务的负载均衡。 负载均衡算法轮询法Round Robin 为了避免可能出现的并发问题，如数组越界，通过新建方法内的局部变量 serverMap，先将域变量复制到线程本地，避免被多个线程修改。但在这一轮选择服务器的过程中，新增服务器或者下线服务器，负载均衡算法中将无法获知。 对于当前轮询的位置变量 pos，为了保证服务器选择的顺序性，需要在操作时对其加上 synchronized 锁，使得在同一时刻只有一个线程能够修改 pos 的值，否则当 pos 变量被并发修改，则无法保证服务器选择的顺序性，甚至可能导致 keyList 数组越界。 使用轮询策略的目的在于，希望做到请求转移的绝对平衡，但付出的性能代价也是相当大的。为了 pos 保证变量修改的互斥性，需要引入重量级的悲观锁 synchronized，将会导致该段轮询代码的并发吞吐量发生明显的下降。 随机法Random 源地址哈希法源地址哈希法的思想是获取客户端访问的IP地址值，通过哈希函数计算的到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是要访问的服务器序号。采用 哈希法 进行负载均衡，同一 IP 地址的客户端，当后端服务器列表不变时，它每次都会被映射到同一台后端服务器进行访问。 加权轮询法Weight Round Robin不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此他们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请求，而低配置、负载高的机器，则给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。 加权随机法Weight Random 最小连接数法Least Connections 动态配置规则固定的策略有些时候还是无力满足千变万化的需求，对于开发者来说，一方面需要支持特定用户的独特需求，另一方面又得尽可能的复用代码，避免重复开发，导致维护成本增加，这边需要将这部分特殊的需求剥离出来，采用动态配置的方式来实现。Groovy 脚本：规则动态加载与执行：1234GroovyClassLoader groovyClassLoader = new GroovyClassLoader(Thread.currentThread().getContextClassLoader());cClass&lt;?&gt; groovyClass = groovyClassLoader.parseClass(sourceCode);GroovyObject groovyObject = (GroovyObject) groovyClass.newInstance();String server = (String) groovyObject.invokeMethod("execute", serverWeightMap); Groovy 实现的随机算法：1234567891011121314151617class DynamicRule &#123; def execute(serverListMap) &#123; def serverMap = [:]; serverMap.putAll(serverListMap); // 取得 IP 地址 list def keySet = serverMap.keySet(); def keyList = new ArrayList&lt;String&gt;(); keyList.addAll(keySet); def random = new Random(); def randomPos = random.nextInt(keyList.size()); String server = keyList.get(randomPos); return server; &#125;&#125; Zookeeper 与 环境搭建Zookeeper 是 Hadoop 下的一个子项目，它是一个针对大型分布式系统的可靠的协调系统，提供的功能包括配置维护、名字服务、分布式同步、组服务等。 ZK 是可以集群复制的，集群间通过 Zab (Zookeeper Atomic Broadcas) 协议来保持数据的一致性。该协议看起来像是 Paxos 协议的某种变形，该协议包括两个阶段：leader election 阶段和 Atomic broadcas 阶段。 集群中将选举出一个 leader，其他的机器则称为 follower，所有的写操作都传给 leader，并通过 broadcas 将所有的更新告诉 follower。当 leader 崩溃或者 leader 失去大多数的 follower 时，需要重新选举出一个新的 leader，让所有服务器都恢复到一个正确的状态。当 leader 被选举出来，且大多数服务器完成了和 leader 的状态同步后，leader election 的过程就结束了，将进入 Atomic broadcas 的过程。Atomic broadcas 同步 leader 和 follower 之间的信息，保证 leader 和 follower 具有相同的系统状态。 Zookeeper API 使用 Zookeeper 实现了一个层次命名空间的数据模型，也可以认为它就是一个小型的、精简的文件系统。它的每个节点称为 znode，znode 除了本身能够包含一部分数据之外，还能够拥有子节点，当节点上的数据发生变化，或者其子节点发生变化时，基于 watcher 机制，会发出相应的通知给订阅状态变化的客户端。 创建节点创建节点示例：123Zookeeper zookeeper = new Zookeeper(url, sessionTimeOut, null);// 创建 /root 节点，其包含的数据为“root data”，访问权限开放，所有人均可以访问，创建模式为持久化节点zookeeper.create("/root", "root data".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); 删除节点删除时需要指定节点的版本号 version，如果设置为 -1，则匹配所有版本，Zookeeper 会比较删除的节点版本是否和服务器上的版本一致，如果不一致则抛出异常。1zookeeper.delete("/root", -1); 设置和获取节点内容一个 znode 中最多能够保存 1MB 的数据设置和获取节点内容的示例：12345// 设置 /root 节点的数据，版本号为-1，如果匹配不到响应的节点，会抛出异常zooKeeper.setData("/root", "hello".getBytes(), -1);// 取得 /root 节点的数据，并返回其 statStat stat = new Stat();byte[] data = zooKeeper.getData("/root", false, stat); 添加子节点Zookeeper 支持在已有的节点下添加子节点，同样也是使用 create 方法，但是父节点必须存在，否则会抛出异常。创建子节点的示例：12zookeeper.create("/root", "root data".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);zookeeper.create("/root/child", "child data".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); 判断节点是否存在123456Stat stat = zooKeeper.exists("/root/child1", false);if (stat == null) &#123; System.out.println("节点不存在");&#125; else &#123; System.out.println("节点存在");&#125; watcher 的实现当节点的状态发生变化，通过 watcher 机制，可以让客户端得到通知，watcher 需要实现 org.apache.ZooKeeper.Watcher 接口。watcher 实现示例：123456789101112131415161718192021public class ZKWatcher implements Watcher &#123; @Override public void process(WatchedEvent event) &#123; if (event.getType() == EventType.NodeDeleted) &#123; System.out.println("node delete"); &#125; if (event.getType() == EventType.NodeChildrenChanged) &#123; System.out.println("node NodeChildrenChanged"); &#125; if (event.getType() == EventType.NodeCreated) &#123; System.out.println("node NodeCreated"); &#125; if (event.getType() == EventType.NodeDataChanged) &#123; System.out.println("node NodeDataChanged"); &#125; &#125;&#125; 需要注意的是，ZK 的 watcher 是一次性的，也就是说，每次在处理完状态变化事件之后，需要重新注册 watcher。这个特性使得在处理事件和重新加上 watcher 这段时间发生的节点状态变化将无法被感知。 ZK 常常发生的两种状态异常： org.apache.ZooKeeper.KeeperException.ConnectionLossException，客户端与其中一台服务器 socket 连接出现异常，连接丢失 org.apache.ZooKeeper.KeeperException.SessionExpiredException，客户端的 session 已经超过 sessionTimeout，未进行任何操作 zkClient 的使用zkClient 解决了 watcher 的一次性注册问题，将 znode 的事件重新定义为子节点的变化、数据的变化、连接及状态的变化三类。zkClient 统一将 watcher 的 WatchedEvent 转换到以上三种情况去处理，watcher 执行后重新读取数据的同时，再注册相同的 watcher。zkClient 在发生 session expire 异常时会自动创建新的 ZooKeeper 实例进行重连。zkClient 对 ZooKeeper 的基本 API 做了一些封装，使用起来更加简洁：1234567891011121314151617ZkClient zkClient = new ZkClient(serverList);// 创建节点zkClient.createPersistent(PATH);// 创建字节点zkClient.create(PATH + "/child", "child znode", CreateMode.EPHEMERAL);// 获得子节点List&lt;String&gt; children = zkClient.getChildren(PATH);// 获得子节点个数int childCount = zkClient.countChildren(PATH);// 判断节点是否存在zkClient.exists(PATH);// 写入数据zkClient.writeData(PATH + "/child", "hello everyone");// 读取节点数据Object obj = zkClient.readData(PATH + "/child");// 删除节点zkClient.delete(PATH + "/child"); 路由和负载均衡的实现 一旦服务器与 ZooKeeper 集群断开连接，节点也就不存在了，通过注册相应的 watcher，服务消费者能够第一时间获知服务提供者机器信息的变更。利用其 znode 的特点和 watcher 机制，将其作为动态注册和获取服务信息的配置中心，统一管理服务名称和其服务器对应的服务器列表信息，我们能够近乎实时地感知到后端服务器的状态（上线、下线、宕机）。ZooKeeper 集群间通过 Zab 协议，服务器配置信息能够保持一致，而 ZooKeeper 本身容错特性和 leader 选举机制，能保障我们方便地进行扩容。 基于 ZooKeeper 所实现的服务消费者获取服务提供者地址列表的部分关键代码：12345678910111213141516171819String serviceName = "service-B";String zkServerList = "192.168.136.130:2181";String SERVICE_PATH = "/configcenter/" + serviceName; // 服务节点路径ZkClient zkClient = new ZkClient(zkServerList);boolean serviceExists = zkClient.exists(SERVICE_PATH);if (serviceExists) &#123; // 服务存在，取地址列表 serverList = zkClient.getChildren(SERVICE_PATH);&#125; else &#123; throw new RuntimeException("service not exist!");&#125;// 注册时间监听zkClient.subscribeChildChanges(SERVICE_PATH, new IZkChildListener()) &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; serverList = currentChilds; &#125;&#125; 服务提供者向 ZooKeeper 集群注册服务的部分关键代码：123456789101112131415161718String serverList = "192.168.136.130:2181";String PATH = "/configcenter"; // 根节点路径ZkClient zkClient = new ZkClient(serverList);boolean rootExists = zkClient.exists(PATH);if(!rootExists) &#123; zkClient.createPersistent(PATH);&#125;boolean serviceExists = zkClient.exists(PATH + "/" + serviceName);if(serviceExists) &#123; zkClient.createPersistent(PATH + "/" + serviceName); // 创建服务节点&#125;// 注册当前服务器，可以在节点的数据里面存放节点的权重InetAddress addr = InetAddress.getLocalHost();String ip = addr.getHostAddress().toString(); // 获得本机 IP// 创建当前服务器节点zkClient.createEphemeral(PATH + "/" + serviceName + "/" + ip); HTTP 服务网关网关（gateway）接收外部各种 APP 的 HTTP 请求，完成相应的权限与安全校验。当校验通过后，根据传入的服务名称，到服务配置中心找到相应的服务名称节点，并加载对应服务提供者的地址列表，通过前面所提到的负载均衡算法，选取服务器发起远程调用，将客户端参数传递到后端服务端。服务提供方根据所传入的参数，给出正确的响应，当 gateway 接收到响应后，再将响应输出给客户端 APP。 Gateway 能够很好地解决安全问题，在恶意请求或者非授权请求到达后端服务器之前进行拦截和过滤。 Gateway 通过服务名称进行服务的路由和负载均衡调度，使得不同的平台之间能够很好地复用公共的业务逻辑，降低了开发和运维成本。 对于外部的 APP 来说，它依赖 gateway 进行服务的路由以及请求的转发，gateway 是整个网络的核心节点，一旦 gateway 失效，所有依赖它的外部 APP 都将无法使用。并且，由于所有的请求均经过 gateway 进行安全校验和请求转发，其流量是整个后端集群流量之和。因此，在设计之初，就需要考虑到系统流量的监控和容量规划，以及 gateway 集群的可扩展性，以便在流量达到极限之前，能够快速方便地进行系统扩容。]]></content>
      <tags>
        <tag>SOA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统基础设施]]></title>
    <url>%2F2019%2F09%2F18%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%2F</url>
    <content type="text"><![CDATA[一个大型、稳健、成熟的分布式系统的背后，往往会涉及众多的支撑系统，我们将这些支撑系统称为分布式系统的基础设施。 分布式缓存由于单台机器的内存资源和承载能力有限，并且如果大量使用本地缓存，也会使相同的数据被不同的节点存储多份，对内存资源造成较大的浪费，因此才催生出了分布式缓存。 memcache用于在应用中减少对数据库的访问，提高应用的访问速度，并降低数据库的负载。memcache 使用 key-value 形式存储和访问数据，在内存中维护一张巨大的 HashTable。memcache 使用了 libevent 来进行高效的网络连接处理。 memcache API 与分布式memcache 客户端与服务端通过构建在 TCP 协议之上的 memcache 协议来进行通信，协议支持两种数据的传递，这两种数据分别为文本行和非结构化数据。 文本行：主要用来承载客户端的命令及服务端的响应。 非结构化数据：主要用于客户端和服务端数据的传递。采用字节流的形式。 memcache 协议支持通过如下几种方式来读取写入失效数据： set 如果存在同样的 key ，替换 add 如果存在同样的 key ，失败 replace 如果不存在 key ，失败 append prepend cas 提供对变量的 cas 操作，它将保证数据更新之前，数据没有被其他人修改; get incr decr delete memcache 官方提供的 Memcache-Java-Client 工具包含了对 memcache 协议的 Java 封装，使用它可以比较方便地与缓存服务端进行通信，它的初始化方式如下：1234567891011121314151617public static void init() &#123; String[] servers = &#123; "192.168.136.135:11211" &#125;; SockIOPool pool = SockIOPool.getInstance(); pool.setServers(servers); // 设置服务器 pool.setFailover(true); // 容错 pool.setInstance(10); // 设置初始连接数 pool.setMinConn(25); // 设置最小连接数 pool.setMaxConn(25); // 设置最大连接数 pool.setMaintSleep(30); // 设置连接池维护线程的睡眠时间 pool.setNagle(false); // 设置是否使用 Nagle 算法 pool.setSocketTO(true); // 设置 socket 的读取等待超时时间 pool.setAliveCheck(true); // 设置连接心跳检测开关 pool.setHashingAlg(SockIOPool.CONSISTENT_HASH); // 设置 Hash 算法 pool.initialize();&#125; memcache 本身并不是一种分布式缓存系统，它的分布式是由访问它的客户端来实现的。 一种比较简单的实现方式是根据缓存的 key 来进行 Hash ，当后端有 N 台缓存服务器时，访问的服务器为 hash(key)%N，这样可以将前端的请求均衡地映射到后端的缓存服务器。 但如果考虑缓存实例变动（增删）的情况：某一缓存实例宕机，需要将该实例从集群中摘除，则映射公式变为 hash(object) % (N - 1)增加一台缓存实例，将该实例加入集群，则映射公式变为 hash(object) % (N + 1)对于以上情况，无论新增还是移除，大部分object所映射的缓存实例均会改变，缓存命中率大幅度降低从而回源到服务器，短时间内造成缓存雪崩现象。 使用 consistent Hash 算法能够在一定程度上改善上述问题。 环形结构 虚拟节点 分布式 session 传统的应用服务器，如 tomcat、jboss 等，其自身所实现的 session 管理大部分都是基于单机的。对于大型分布式网站来说，支撑其业务的远远不止一台服务器，而是一个分布式集群，请求在不同服务器之间跳转。传统网站一般将一部分数据存储在 cookie 中，来规避分布式环境下 session 的操作。这样做的弊端很多，一方面 cookie 的安全性一直广为诟病，另一方面 cookie 存储数据的大小是有限制的。 将 session 持久化到 DB 中，可以保证宕机时会话不易丢失，但系统的整体吞吐将受到很大的影响。 将session 统一存储在缓存集群上，可以保证较高的读写性能，可以利用缓存的失效机制，但一旦缓存重启，里面保存的会话也就丢失了。 通过将 session 以 sessionid 作为 key，保存到后端的缓存集群中，使得不管请求如何分配，即便是 Web Server 宕机，也不会影响其他 Web Server 通过 sessionid 从 Cache Server 中获得 session，这样既实现了集群间的 session 同步，又提高了 Web Server 的容错性。 memcache-session-manager 是一个开源的高可用的 Tomcat session 共享解决方案，支持 Sticky 模式和 Non-Sticky 模式。以 Non-Sticky 模式为例，它需要给 Tomcat 的 $CATALINA_HOME/conf/context.xml 文件配置 SessionManager，具体配置如下：12345678&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.0.100,:11211, n2:192.168.0.101:11211" Sticky="false" sessionBackupAsync="false" lockingNode="auto" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$" transcoderFactoryClass = "de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"/&gt; 持久化存储面对并发访问量的激增和数据几何级的增长，如何存储正在迅速膨胀并且不断累积的数据，以及应对日益增长的用户访问频次，称为亟待解决的问题。 传统IOE解决方案，使用和扩展的成本越来越高，使得互联网企业不得不思考新的解决方案。开源软件加廉价PC Server的分布式架构，得益于社区的支持。在节约成本的同时，也给系统带来了良好的扩展能力，并且由于开源软件的代码透明，使得企业能够以更低的代价定制更符合自身使用场景的功能，以提高系统的整体性能。 MySQL 扩展业务拆分 业务发展初期为了便于快速迭代，很多应用都采用集中式的架构。随着业务规模的扩展，使系统变得越来越复杂，越来越难以维护，开发效率越来越低，并且系统的资源消耗也越来越大，通过硬件提升性能的成本也越来越高。因此，系统业务的拆分是难以避免的。 业务拆分不仅仅提高了系统的可扩展性，也带来了开发工作效率的提升。 复制策略要实现数据库的复制，需要开启 Master 服务器端的 Binary log。数据复制的过程实际上就是 Slave 从 Master 获取 binary log，然后再在本地镜像的执行日志中记录的操作。由于复制过程是异步的，只能够保证数据最终一致性。MySQL 的复制可以基于一条语句(statement level)，也可以基于一条记录(row level)。 前段服务器通过 Master 来执行数据写入的操作，数据的更新通过 Binary log 同步到 Slave 集群，而对于数据读取的请求，则交由 Slave 来处理，这样 Slave 集群可以分担数据库读的压力，并且读、写分离保障了数据能够达到最终一致性。 Q:Master-Slaves 复制架构存在一个问题，即所谓的单点故障。当 Master 宕机时，系统将无法写入，而在某些特定的场景下，也可能需要 Master 停机，以便进行系统维护、优化或者升级。A:最佳的方式就是采用 Dual-Master 架构，即 Master-Master 架构。MySQL 不会将复制产生的变更记录到 Binary log ，这样就避免了服务器间数据的循环复制。 仅开启一台 Master 的写入，另一台 Master 仅仅 stand by 或者作为读库开放，这样可以避免数据写入的冲突，防止数据不一致的情况发生。 如需进行停机维护，可按如下步骤执行 Master 的切换操作： 停止当前 Master 的所有写入操作。 在 Master 上执行 set global read_only=1，同时更新 MySQL 配置文件中相应的配置，避免重启时失效。 在 Master 上执行 show Master status，以记录 Binary log 坐标。 使用 Master 上的 Binary log 坐标，在 stand by 的 Master 上执行 select Master_pos_wait()，等待 stand by Master 的 Binary log 跟上 Master 的 Binary log。 在 stand by Master 开启写入时，设置 read_only=0。 修改应用程序的配置，使其写入到新的 Master。 假如 Master 意外宕机，处理过程要稍微复杂一点，因为此时 Master 与 stand by Master 上的数据并不一定同步，需要将 Master 上没有同步到 stand by Master 的 Binary log 复制到 Master 上进行 replay，直到 stand by Master 与 原 Master 上的 Binary log 同步，才能够开启写入；否则，这一部分不同步的数据就有可能导致数据不一致。 分表与分库对于访问极为频繁且数据量巨大的单表来说，我们首先要做的就是减少单表的记录条数，以便减少数据查询所需要的时间，提高数据库的吞吐，这就是所谓的分表。 用户 id 是最常用的分表字段。 假设有一张记录用户购买信息的订单表 order，由于 order 表记录条数太多，将被拆分成 256 张表。拆分的记录根据 user_id%256，找到对应订单存储的表进行访问。 分表能够解决单表数据量过大带来查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。与分表策略相似，分库也可以采用通过一个关键字取模的方式，来对数据访问进行路由。 有时数据库可能既面临着高并发访问的压力，又需要面对海量数据的存储问题，这时需要对数据库即采用分库策略，又采用分表策略，以便同时扩展系统的并发处理能力，以及提升单表的查询能力，这就是所谓的分库分表。 一般分库分表的路由策略： 中间变量=user_id%(库数量*每个库的表数量) 库=取整(中间变量/每个库的表数量) 表=中间变量%每个库的表数量 业务拆分及分库分表带来的问题： 原本跨表的事务上升为分布式事务； 由于记录被切分到不同的库与不同的表当中，难以进行多表关联查询，并且不能不指定路由字段对数据进行查询。 如果需要对系统进行进一步扩容(路由策略变更)，将变得非常不方便，需要重新进行数据迁移。 HBaseApache Hadoop 项目下的一个子项目，它以 Google BigTable 为原型，设计实现了高可靠性、高可扩展性、实时读写的列存储数据库。 本质实际上是一张稀疏的大表，用来存储粗粒度的结构化数据，并且能够通过简单地增加节点来实现系统的线性扩展。 HBase 运行在分布式文件系统 HDFS 之上，利用它可以在廉价的 PC Server 上搭建大规模结构化存储集群。 HBase 集群中通常包含两种角色，HMaster 和 HRegionServer。当表随着记录条数的增加而不断变大后，将会分裂成一个个Region，每个 Region 可以由(startkey，endkey)来表示，它包含一个 startkey 到 endkey 的半闭区间。一个 HRegion 可以管理多个 Region，并由 HMaster 来负责 HRegionServer 的调度及集群状态的监管。 HBase 安装 下载 编辑 hbase-env.sh ，设置 JAVA_HOME 编辑 hbase-site.xml 文件 启动 HBase 先启动 Hadoop 再启动 HBase HBase API以 Java 为例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/*** 配置 HBase 的 HMaster 服务器地址和对应的端口（默认为60000），以及对应的 ZooKeeper 服务器地址和端口*/private static Configuration conf = null;static &#123; conf = HBaseConfiguration.create(); conf.set("hbase.ZooKeeper.property.clientPort", "2181"); conf.set("hbase.ZooKeeper.quorum", "192.168.136.135:60000"); conf.set("hbase.master", "192.168.136.135:60000");&#125;/*** 通过程序增加 user 表，包含三个列族，分别为 info、class、parent，如果该表已经存在，则先删除该表*/public static void createTable() throws Exception &#123; String tableName = "user"; HBaseAdmin hBaseAdmin = new hBaseAdmin(conf); if (hBaseAdmin.tableExists(tableName)) &#123; hBaseAdmin.disableTable(tableName); hBaseAdmin.deleteTable(tableName); &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(tableName.valueOf(tableName)); tableDescriptor.addFamily(new HColumnDescriptor("info")); tableDescriptor.addFamily(new HColumnDescriptor("class")); tableDescriptor.addFamily(new HColumnDescriptor("parent")); hBaseAdmin.createTable(tableDescriptor); hBaseAdmin.close();&#125;/*** 将数据添加到 user 表，每个列族指定一个列 col, 并给该列赋值*/public static void putRow() throws Exception &#123; String tableName = "user"; String[] familyNames = &#123;"info", "class", "parent"&#125;; HTable table = new HTable(conf, tableName); for (int i = 0; i &lt; 20; i++) &#123; for (int j = 0; j &lt; familyNames.length; j++) &#123; Put put = new Put(Bytes.toBytes(i + "")); put.add(Bytes.toBytes(familyNames[j]), Bytes.toBytes("col"), Bytes.toBytes("value_" + i + "_" + j)); table.put(put); &#125; &#125; table.close();&#125;/*** 取得 rowkey 为 1 的行，并将该行打印出来*/public static void getRow() throws IOException &#123; String tableName = "user"; String rowKey = "1"; HTable table = new HTable(conf, tableName); Get g = new Get(Bytes.toBytes(rowKey)); Result r = table.get(g); outputResult(r); table.close();&#125;public static void outputResult(Result rs) &#123; List&lt;Cell&gt; list = rs.listCells(); System.out.println("row key: " + new String(rs.getRow())); for (Cell cell : list) &#123; System.out.println("family: " + new String(cell.getFamily()) + ", col: " + new String(cell.getQualifier()) + ", value: " + new String(cell.getValue()) ); &#125;&#125;/*** scan 扫描 user 表，并将查询结果打印出来*/public static void scanTable() throws Exception &#123; String tableName = "user"; HTable table = new HTable(conf, tableName); Scan a = new Scan(); ResultScanner rs = table.getScanner(s); for (Result r : rs) &#123; outputResult(r); &#125; // 设置 startrow 和 endrow 进行查询 s = new Scan("2".getBytes(), "6".getBytes()); rs = table.getScanner(s); for (Result r : rs) &#123; outputResult(r); &#125; table.close();&#125;/*** 删除 rowkey 为 1 的记录*/public static void deleteRow() throws IOException &#123; String tableName = "user"; String rowKey = "1"; HTable table = new HTable(conf, tableName); List&lt;Delete&gt; list = new ArrayList&lt;Delete&gt;(); Delete d = new Delete(rowkey.getBytes()); list.add(d); table.delete(list); table.close();&#125; rowkey 设计要想访问 HBase 的行，只有三种方式，一种是通过指定 rowkey 进行访问，另一种是指定 rowkey 的 range 进行 scan，再者就是全盘扫描。 关系型数据库能够很好地支持多条件查询，但对于 HBase 来说，实现起来并不是那么的容易。基本的思路就是建立一张二级索引表，将查询条件设计成二级索引表的rowkey，而存储的数据则是数据表的rowkey，这样就可以在一定程度上实现多个条件的查询。但是二级索引表也会引入一系列的问题，多表的插入将降低数据写入的性能，并且由于多表之间无事务保障，可能会带来数据一致性问题。(华为提供了 hindex 的二级索引解决方案) 与传统的关系型数据库相比，HBase 有更好的伸缩能力，更适合于海量数据的存储和处理。由于多个 Region Server 的存在，使得 HBase 能够多个节点同时写入，显著提高了写入性能，并且是可扩展的。 Redis与其他很多 key-value 数据库的不同之处在于，Redis 不仅支持简单的键值对类型的存储，还支持其他一系列丰富的数据存储结构，并在这些数据结构类型上定义了一套强大的 API。 安装 Redis使用 Redis APIRedis 的 Java client 有很多，比较常用的是 Jedis。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Jedis jedis = new Jedis("192.168.136.135", 6379);// 基本操作redis.set("name", "yeonsea"); // 设置 key-valueredis.setex("content", 5, "hello"); // 设置 key-value 有效期为 5 秒redis.mset("class", "a", "age", "25"); // 一次设置多个 key-valueredis.append("content", " lucy"); // 给字符串追加内容String content = redis.get("content"); // 根据 key 获取 value List&lt;String&gt; list = redis.mget("class", "age"); // 一次获取多个 key// hashs 操作redis.hset("url", "google", "www.google.com"); // 给 Hash 添加 key-valueredis.hset("url", "taobao", "www.taobao.com");redis.hset("url", "sina", "www.sina.com.cn");Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();map.put("name", "yeon");map.put("sex", "male");map.put("age", "100");redis.hmset("userinfo", map); // 批量设置值String name = redis.hget("userinfo");// 取 Hash 的多个 key 的值List&lt;String&gt; urlist = redis.hmget("url", "google", "taobao", "sina");// 取 Hash 的所有 key 的值Map&lt;String, String&gt; userinfo = redis.hgetAll("userinfo");// lists 操作redis.lpush("charlist", "abc"); // 在 list 首部添加元素redis.rpush("charlist", "hij"); // 在 list 尾部添加元素List&lt;String&gt; charlist = redis.lrange("charlist", 0, 2);redis.lpop("charlist"); // 在 list 首部删除元素redis.rpop("charlist"); // 在 list 尾部删除元素Long charlistSize = redis.llen("charlist"); // 获得 list 的大小// setsredis.sadd("SetMem", "s1"); // 给 set 添加元素redis.srem("SetMem", "s1"); // 从 set 中移除元素Set&lt;String&gt; set = redis.smember("SetMem"); // 枚举出 set 的元素//sorted sets 是 set 的一个升级版本redis.zadd("SortSetMem", 1, "5th"); // 插入 sort set，并指定元素的序号redis.zadd("SortSetMem", 2, "4th");redis.zadd("SortSetMem", 3, "3th");redis.zadd("SortSetMem", 4, "2th");redis.zadd("SortSetMem", 5, "1th");// 根据范围取 setSet&lt;String&gt; sorted = redis.zrange("SortSetMem", 2, 4);// 根据范围反向取 setSet&lt;String&gt; revsorset = redis.zrevrange("SortSetMem", 1, 2); 消息系统 在分布式系统中，消息系统的应用十分广泛，消息可以作为应用间通信的一种方式，消息被保存在队列中，直到被接收者取出。由于消息发送者不需要同步等待消息接收者的响应，消息的异步接收降低了系统集成的耦合度，提升了分布式系统的协作效率，使得系统能够更快的响应用户，提供更高的吞吐。当系统处于峰值压力时，避免整个系统被压垮。 ActiveMQ &amp; JMSActiveMQ 是 Apache 所提供的一个开源的消息系统，安全采用 Java 实现，因此，它能够很好地支持 J2EE 提出的 JMS 规范。 JMS（Java Message Service，即 Java 消息服务）是一组 Java 应用程序接口，它提供消息的创建、发送、接收、读取等一系列服务。JMS 定义了一组公共应用程序接口和响应的语法，类似于 Java 数据库的统一访问接口 JDBC，它是一种与厂商无关的 API ，使得 Java 程序能够与不同厂商的消息组件很好地进行通信。 JMS 支持的消息类型： 简单文本（TextMessage） 可序列化的对象（ObjectMessage） 键值对（MapMessage） 字节流（BytesMessage） 流（StreamMessage） 无有效负载的消息（Message） JMS 支持两种消息发送和接收模型： Point-to-Point（P2P）模型：基于queue Pub/Sub(发布/订阅)模型：定义了如何向一个内容节点发布/订阅消息，这个内容节点称为 topic（主题） 安装 ActiveMQ通过 JMS 访问 ActiveMQ使用 JMS 来完成 ActiveMQ 基于 queue 的点对点消息发送：1234567891011121314ConnectionFactory connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://192.168.136.135:61616");Connection connection = connectionFactory.createConnection();connection.start();Session session = connection.createSession(Boolean.TRUE, Session.AUTO_ACKNOWLEDGE);Destination destination = session.createQueue("MessageQueue");MessageProducer producer = session.createProducer(destination);producer.setDeliveryMode(DeliveryMode.NON_PERISTENT);ObjectMessage message = session.createObjectMessage("hello everyone!");producer.send(message);session.commit(); 基于 queue 的点对点消息接收1234567891011121314151617181920ConnectionFactory connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://192.168.136.135:61616");Connection connection = connectionFactory.createConnection();connection.start();Session session = connection.createSession(Boolean.FALSE, Session.AUTO_ACKNOWLEDGE);Destination destination = session.createQueue("MessageQueue");MessageConsumer consumer = session.createConsumer(destination);while(true) &#123; // 取出消息 ObjectMessage message = (ObjectMessage) consumer.receive(10000); if (null != messgae) &#123; String messageContent = (String) message.getObject(); System.out.println(messageContent); &#125; else &#123; break; &#125;&#125; 通过 JMS 来创建 ActiveMQ 的 topic，并给 topic 发送消息12345678910111213ConnectionFactory connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://192.168.136.135:61616");Connection connection = connectionFactory.createConnection();connection.start();Session session = connection.createSession(Boolean.FALSE, Session.AUTO_ACKNOWLEDGE);Topic topic = session.createTopic("MessageTopic");MessageProducer producer = session.createProducer(topic);producer.setDeliveryMode(DeliveryMode.NON_PERISTENT);TextMessage message = session.createTextMessage();message.setText("message_hello_yeon");producer.send(message); 消息发送到对应的 topic 后，需要将 listener 注册到需要订阅的 topic 上，以便能够接收该 topic 的消息：1234567891011121314151617181920ConnectionFactory connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://192.168.136.135:61616");Connection connection = connectionFactory.createConnection();connection.start();Session session = connection.createSession(Boolean.FALSE, Session.AUTO_ACKNOWLEDGE);Topic topic = session.createTopic("MessageTopic");MessageConsumer consumer = session.createConsumer(topic);consumer.setMessageListener(new MessageListener() &#123; public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println(tm.getText()); &#125; catch (JMSException e) &#123; // &#125; &#125;&#125;) ActiveMQ 集群部署当一个应用被部署到生产环境中，进行容错和避免单点故障是十分重要的，这样可以避免因为单个节点的不可用而导致整个系统的不可用。目前 ActiveMQ 所提供的高可用方案主要是基于 Master-Slave 模式实现的冷备方案，较为常用的包括基于共享文件系统的 Master-Slave 架构和基于共享数据库的 Master-Slave 架构。 当 Master 启动时，它会获得共享文件系统的排它锁，而其他 slave 则 stand-by，不对外提供服务，同时等待获取 Master 的排它锁。假如 Master 连接中断或者发生异常，那么它的排它锁则会立即释放，此时便会有另外一个 Slave 能够争夺到 Master 的排它锁，从而成为 Master，对外提供服务。当之前的故障或者连接中断而丢失排它锁的 Master 重新连接到共享文件系统时，排它锁已经被抢占了，它将作为 Slave 等待，直到 Master 再一次发生异常。基于共享数据库的 Master-Slave 架构同基于共享文件系统的 Master-Slave 架构类似。当 Master 启动时，会先获取数据库某个表的排它锁，而其他 Slave 则 stand-by，等待表锁，直到 Master 发生异常，连接丢失。这时表锁将释放，其他 Slave 将获得表锁，从而成为 Master 并对外提供服务，Master 与 Slave 自动完成切换，完全不需要人工干预。 系统扩展： 垂直扩展：可以提升单 Broker 的处理能力。最直接的方法就是提升硬件性能，再者就是通过调节 ActiveMQ 本身的一些配置来提升系统并发处理的能力，如使用 nio 替代阻塞 I/O，提高系统处理并发请求的能力，或者调整 JVM 与 ActiveMQ 可用的内存空间等。 水平扩展：对于 ActiveMQ 来说，可以采用 broker 拆分的方式，将不相关的 queue 和 topic 拆分到多个 broker，来达到提升系统吞吐能力的目的。 垂直化搜索引擎垂直化的搜索引擎主要针对企业内部的自有数据的检索，而不像 Google 和 Baidu 等搜索引擎平台，采用网络爬虫对全网数据进行抓取，从而建立索引并提供给用户进行检索。在分布式系统中，垂直化的搜索引擎是一个非常重要的角色，它既能满足用户对于全文检索、模糊匹配的需求，解决数据库 like 查询效率低下的问题，又能够解决分布式环境下，由于采用分库分表或者使用 NoSQL 数据库，导致无法进行多表关联或者进行复杂查询的问题。 Lucene 简介几个重要概念： 倒排索引：invert index，也称为反向索引，它将文档中的词作为关键字，建立词与文档的映射关系，通过对倒排索引的检索，可以根据词快速获取包含这个词的文档列表。 分词，又称切词，就是将句子或者段落进行切割，从中提取出包含固定语义的词。对于支持中文搜索的搜索引擎来说，需要一个合适的中文分词工具，以便建立倒排索引。 停止词，这些词没有具体意义，区分度低，搜索引擎对这些词进行索引没有任何意义，因此，停止词需要被忽略掉。 排序，需要将相关度更大的内容排在前面，以便用户能够更快地筛选出有价值的内容。 Lucene 的几个概念： 文档（Document） 域（Field） 词（Term） 查询（Query），最基本的查询可能是一系列 Term 的条件组合，称为TermQuery，但也有可能是短语查询（PhraseQuery）、前缀查询（PrefixQuery）、范围查询（包括 TermRangeQuery、NumericRangeQuery等） 分词器（Analyzer），文档被索引之前，需要经过分词器处理，以提取关键的语义单元，建立索引，并剔除无用的信息，如停止词等，以提高查询的准确性。 Lucene 的使用构建索引在执行搜索之前，先要构建搜索的索引：12345678910Directory dir = FSDirectory.open(new File(indexPath));Analyzer analyzer = new StandardAnalyzer();Document doc = new Document();doc.add(new Field("name", "yeon", Store.YES, Index.ANALYZED)); // 需要对该字段进行全文检索doc.add(new Field("address", "kk", Store.YES, Index.ANALYZED));doc.add(new Field("sex", "male", Store.YES, Index.NOT_ANALYZED)); // 不进行全文检索doc.add(new Field("introduce", "xxxxxxxx", Store.YES, Index.NO)); // 不进行检索IndexWriter indexWriter = new IndexWriter(dir, analyzer, MaxFieldLength.LIMITED);indexWriter.addDocument(doc);indexWriter.close(); 索引更新与删除Lucene 暂时还不支持对于 Document 单个 Field 或者整个 Document 的更新。因此这里的更新，实际上是删除旧的 Document ，然后再向索引中添加新的 Document。所添加的新的 Document 必须包含所有的 Field，包括没有更改的 Field。123IndexWriter indexWriter = new IndexWriter(dir, analyzer, MaxFieldLength.LIMITED);indexWriter.deleteDocuments(new Term("name", "yeon")); // 一个不正确的 Term 可能导致搜索引擎的大量索引被误删indexWriter.addDocument(doc); Lucene 也提供经过封装的 updateDocument 方法，实质还是一样。1indexWriter.updateDocument(new Term("name", "yeon"), doc); 条件查询123456789101112131415161718String queryStr = "yeon";String[] fields = &#123;"name", "introduce"&#125;;Analyzer analyzer = new StandardAnalyzer();QueryParser queryParser = new MultiFieldQueryParser(fields, analyser);Query query = queryParser.parse(queryStr);IndexSearcher indexSearcher = new IndexSearcher(indexPath);Filter filter = null;TopDocs topDocs = indexSearcher.search(query, filter, 10000);System.out.println("hits :" + topDocs.totalHits);for(ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; int docNum = scoreDoc.doc; Document doc = indexSearcher.doc(docNum); printDocumentInfo(doc);&#125; 针对某个 Field 关键字查询：12Term term = new Term("name", "yeon");Query termQuery = new TermQuery(term); 针对某个范围对 Field 的值进行区间查询：1NumericRangeQuery numericRangeQuery = NumericRangeQuery.newIntRange("size", 2, 100, true, true); 通过通配符来对 Field 进行查询12Term wildcardTerm = new Term("name", "yeo?"); // ? 0个或1个字母；* 0个或多个字母WildCardQuery wildCardQuery = new WildCardQuery(wildcardTerm); 通过关键字查询1234PhraseQuery phraseQuery = new PhraseQuery();phraseQuery.add(new Term("content", "dog"));phraseQuery.add(new Term("content", "cat"));phraseQuery.setSlop(5); // 两个短语之间最多不超过 5 个单词 将不同条件组合起来进行复杂查询12345678910PhraseQuery query1 = new PhraseQuery();query1.add(new Term("content", "dog"));query1.add(new Term("content", "cat"));query1.setSlop(5);Term wildTerm = new Term("name", "yeo?");WildCardQuery query2 = new WildCardQuery(wildTerm);BooleanQuery booleanQuery = new BooleanQuery();booleanQuery.add(query1, Occur.MUST);booleanQuery.add(query2, Occur.MUST); // 表示符合条件的 Document 才被包含在结果中 结果排序1234567891011121314151617String queryStr = "lishi";String[] fields = &#123;"name", "address", "size"&#125;;Sort sort = new Sort();SortField field = new SortField("size", SortField.INT, true);sort.setSort(field);Analyzer analyzer = new StandardAnalyzer();QueryParser queryParser = new MultiFieldQueryParser(fields, analyzer);Query query = queryParser.parse(queryStr);IndexSearcher indexSearcher = new IndexSearcher(indexPath);Filter filter = null;TopDocs topDocs = indexSearcher.search(query, filter, 100, sort);for(ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; int docNum = scoreDoc.doc; Document doc = indexSearcher.doc(docNum); printDocumentInfo(doc);&#125; 如果多个 Field 同时进行查询，可以指定每个 Field 拥有不同的权重，以便匹配时可以按照 Document 的相关度进行排序。123456789101112131415161718String queryStr = "zhangsan shanghai";String[] fields = &#123;"name", "address", "size"&#125;;Map&lt;String, Float&gt; weights = new HashMap&lt;String, Float&gt;();map.put("name", 4f);map.put("address", 2f);Analyzer analyzer = new StandardAnalyzer();QueryParser queryParser = new MultiFieldQueryParser(fields, analyzer, weights);Query query = queryParser.parse(queryStr);IndexSearcher indexSearcher = new IndexSearcher(indexPath);Filter filter = null;TopDocs topDocs = indexSearcher.search(query, filter, 100);for(ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; int docNum = scoreDoc.doc; Document doc = indexSearcher.doc(docNum); printDocumentInfo(doc);&#125; 高亮查询到匹配的文档后，需要对匹配的内容进行突出展现，最直接的方式就是对匹配的内容高亮显示。123456Formatter formatter = new SimpleHTMLFormater("&lt;font color='red'&gt;", "&lt;/font&gt;");Scorer scorer = new QueryScorer(query);Highlighter highLight = new Highlighter(formatter, scorer);Fragmenter fragmenter = new SimpleFragmenter(20);highLight.setTextFragmenter(fragmenter);String hi = highLight.getBestFragmenter(analyzer, "introduce", doc.get("introduce")); 中文分词Lucene 提供的标准中文分词器 StandardAnalyzer 只能够进行简单的一元分词。常用的中文分词器包括 Lucene 自带的中日韩文分词器 CJKAnalyzer，国内也有一些开源的中文分词器，包括 IK 分词、MM 分词、以及庖丁分词、imdict 分词器等。假设有下面一段文字：1String zhContent = "我是一个中国人，我热爱我的国家"; 分词之后，通过下面一段代码可以将分词的结果打印输出：12345678910111213System.out.println("\n 分词器：" + analyze.getClass());TokenStream tokenStream = analyze.tokenStream("content", new StringReader(text));Token token = tokenStream.next();while (token != null) &#123; System.out.println(token); token = tokenStream.next();&#125;// 分词器Analyzer standardAnalyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);Analyzer cjkAnalyzer = new CJKAnalyzer();Analyzer ikAnalyzer = new IKAnalyzer();Analyzer mmAnalyzer = new MMAnalyzer(); 索引优化Lucene 的索引是由段（segment）组成的，每个段可能又包含多个索引文件，即每个段包含了一个或多个 Document；段结构使得 Lucene 可以很好地支持增量索引，新增的 Document 将被添加到新的索引段当中。一般来说，操作系统对于进程打开的文件句柄是有限的，当一个进程打开太多的文件时，会抛出 too many open files 异常，并且执行搜索任务时，Lucene 必须分别搜索每个段，然后将各个段的搜索结果合并，这样查询的性能就会降低。 一般来说，在分布式环境下，会安排专门的集群来生成索引，并且生成索引的集群不负责处理前台的查询请求。当索引生成以后，通过索引优化，对索引的段进行合并。合并完以后，将生成好的索引文件分发到提供查询服务的机器供前台应用查询。 分布式扩展与其他应用不同的是，搜索应用大部分场景都能够接受一定时间内的数据延迟，对于数据一致性的要求并不那么高，大部分情况下只要能保障数据的最终一致性，可以容忍一定时间上的数据不同步。 SolrSolr 是一个基于 Lucene 、功能强大的搜索引擎工具，它对 Lucene 进行了扩展，提供一系列功能强大的 HTTP 操作接口，支持通过 Data Scheme 来定义字段、类型和设置文本分析，使得用户可以通过 HTTP POST 请求，向服务器提交 Document，生成索引，以及进行索引的更新和删除操作。对于复杂的查询条件，Solr 提供了强大的可配置能力，以及功能完善的后台管理系统。 Solr 的配置修改 Tomcat 的 conf/server.xml 中的 Connector 配置，将 URIEncoding 编码设置为 UTF-8，否则中文将会乱码，从而导致搜索查询不到结果。123&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" URIEncoding="UTF-8"/&gt; 将 Solr 的 dist 目录下的 solr-{version}.war 包复制到 tomcat 的 webapps 目录下，并且重命名为 solr.war。配置 Solr 的 home 目录，包括 scheme 文件、solrconfig 文件及索引文件。 构建索引条件查询]]></content>
      <tags>
        <tag>SOA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网安全架构]]></title>
    <url>%2F2019%2F09%2F11%2F%E4%BA%92%E8%81%94%E7%BD%91%E5%AE%89%E5%85%A8%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[随着移动互联网的兴起，以及 RESTful 和 Web Service 等技术的大规模使用，HTTP 协议因其使用方便及跨平台的特性，在 Web 开发和 SOA 领域得到了广泛使用。但其所涵盖的信息，大都是未经加密的明文，信息获取门槛的降低，也为应用架构的安全性与稳定性带来了挑战。 常见的 Web 攻击手段XSS, CSRF, SQL注入更多需要从开发流程上来予以保障，而DDoS，攻击手段多样，产生的影响及危害巨大。 XSS 攻击跨站脚本攻击(Cross Site Scripting) XSS 原理表单，URL中带有恶意脚本 XSS 防范XSS 之所以会发生，是因为用户输入的数据变成了代码。因此，我们需要对用户输入的数据进行HTML转义处理。如今很多开源的开发框架本身默认就提供HTML代码转义的功能，如流行的jstl,struts等，不需要开发人员再进行过多的开发。1escapeXml=&quot;true&quot; CRSF 攻击跨站请求伪造(cross site request forgery)，是一种对网站的恶意利用。XSS 利用的是站点内的信任用户，而CSRF则是通过伪装来自受信任用户的请求来利用受信任的网站。 CRSF 攻击原理受害者只需要做下面两件事情，攻击者就能够完成CSRF攻击： 登录受信任站点A，并在本地生成cookie; 在不登出站点A(清除站点A的cookie)的情况下，访问恶意站点B 很多情况下所谓的恶意站点，很有可能是一个存在其他漏洞(如XSS)的受信任且被很多人访问的站点，这样，普通用户可能在不知不觉中便成为了受害者。 攻击举例银行转账正常银行的交易付款会有USB key，验证码，登录密码和支付密码等一系列屏障，流程上复杂的多，安全系数也高得多。 CSRF 的防御 将 cookie 设置为 HttpOnly在Java的Servlet的API中设置cookie为HostOnly的代码如下 response.setHeader(“Set-Cookie”, “cookiename=cookievalue;HttpOnly&quot;); 增加 token攻击者可以在不知道用户验证信息的情况下直接利用用户的cookie来通过安全验证。由此可知，抵御CSRF攻击的关键在于：在请求中放入攻击者所不能伪造的信息。token 的值通过服务端生成，表单提交后token的值通过post请求与参数一同带到服务端，每次会话可以使用相同的token，会话过期，则token失效，攻击者无法获取到token，也就无法伪造请求。在 session 中添加 token 的实现代码： 12345HttpSesssion session = request.getSession();Object token = session.getAttribute("_token");if (token == null || "".equals(token)) &#123; session.setAttribute("_token", UUID.randomUUID().toString());&#125; 通过 Referer 识别根据HTTP协议，在HTTP头中有一个字段叫Referer，它记录了该HTTP请求的来源地址。在通常情况下，访问一个安全受限的页面的请求都来自于同一个网站。 String referer = request.getHeader(&quot;Referer&quot;); SQL 注入攻击 所谓 SQL 注入，就是通过把 SQL 命令伪装成正常的 HTTP 请求参数，传递到服务端，欺骗服务器最终执行恶意的 SQL 命令，达到入侵目的。 攻击原理SQL 注入的防范 使用预编译语句 PreparedStatement 使用 ORM 框架 防止SQL注入的关键在于对一些关键字符进行转义 需要一些简单的配置 避免密码明文存放 哈希加盐 处理好相应的异常 后台的系统异常，很可能包含了一些如服务器版本、数据库版本、编程语言等信息，甚至是数据库连接的地址与用户名密码，攻击者可以按图索骥，找到对应版本的服务器漏洞或者数据库漏洞进行攻击。文件上传漏洞 在上网的过程中，我们经常会将一些图片、压缩包之类的文件上传到远端服务器进行保存。文件上传攻击指的是恶意攻击者利用一些站点没有对文件的类型做很好的检验，上传了可以执行的文件或者脚本，并且通过脚本获得服务器上响应的权利，或者是通过诱导外部用户访问，下载上传的病毒或木马文件，达到攻击的目的。很多类型的文件，起始的几个字节内容是固定的，因此，根据这几个字节的内容，就可以确定文件类型，这几个字节也被称为“魔数”。 通过魔数来判断文件类型 imagemagick 针对图片处理的开发工具包 没有提供 jni 对应的头文件(对缩放好的图像进行保存的图片水印生成，锐化，截取，图像格式转换等一系列复杂的操作) jmagic Java 通过它对imagemagick进行调用 (so文件放在/usr/lib下，用jar进行调用) 代码实现：12345678910111213141516171819202122232425262728293031/** * 读取文件头*/private static String getFileHeader(String filePath) throws IOException &#123; // 这里需要注意的是，每个文件的魔数的长度都不相同，因此需要使用 startwith byte[] b = new byte[28]; InputStream inputStream = null; inputStream = new FileInputStream(filePath); inputStream.read(b, 0, 28); inputStream.close(); return bytes2hex(b);&#125;/** * 判断文件类型*/public static FileType getType(String filePath) throws IOException &#123; String fileHead = getFileHeader(filePath); if (fileHead == null || fileHead.length() == 0) &#123; return null; &#125; fileHead = fileHead.toUpperCase(); FileType[] fileTypes = FileType.values(); for (FileType type : fileTypes) &#123; if (fileHead.startsWith(type.getValue())) &#123; return type; &#125; &#125; return null;&#125; DDoS 攻击Distribute Denial of Service 分布式拒绝服务攻击 SYN FloodSYN Flood 利用了TCP协议三次握手的过程来达成攻击的目的。攻击者伪造大量的IP地址给服务器发送SYN报文，但是由于伪造的IP地址几乎不可能存在，也就不可能从客户端得到任何回应，服务端将维护一个非常大的半连接等待列表，并且不断对这个列表中的IP地址进行遍历和重试，占用了大量的系统资源。更为严重的是，由于服务器资源有限，大量的恶意客户端信息占满了服务器的等待队列，导致服务器不再接收新的SYN请求，正常用户无法完成三次握手与服务器进行通信，这便是SYN Flood攻击。 DNS Query Flood是 UDP Flood 攻击的一种变形，攻击方式是向被攻击的服务器发送海量的域名解析请求。 CC 攻击Challenge Collapsar 是基于应用层 HTTP 协议发起的 DDoS 攻击，也称为 HTTP Flood。控制大量肉鸡或HTTP代理，有意避开CDN以及分布式缓存，多次DB，从而拖垮后端的业务处理系统。 其他攻击手段DNS域名劫持、CDN回源攻击、服务器权限提升、缓冲区溢出，平台或软件漏洞。 常用的安全算法数字摘要也称消息摘要，由一个单向 Hash 函数对消息进行计算而产生。一个 Hash 函数的好坏是由发生碰撞的概率决定的，如果攻击者能够轻易地构造出两个具有相同 Hash 值的消息，那么这样的 Hash 函数是很危险的。可以认为，摘要的长度越长，算法也就越安全。 摘要的特点: 无论输入的消息有多长，计算出来的消息摘要长度总是固定的。 一般只要输入的消息不同，对其进行摘要以后产生的摘要消息也不相同，但输入相同必会产生相同的输出。 由于消息摘要并不包含原文的信息，因此只能进行正向的信息摘要，而无法从摘要中恢复出原来的消息，甚至根本就找不到任何与原消息相关的信息。 MD5Message Digest Algorithm 5(信息摘要算法5)，是数字摘要算法的一种实现，用于确保信息传输完整性和一致性，摘要长度为 128 位。MD5由MD4，MD3，MD2改进而来，主要增强了算法复杂度和不可逆性。基于 Java 的 MD5 算法的使用12345public static byte[] testMD5(String content) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("MD5"); byte[] bytes = md.digest(content.getBytes("utf8")); return bytes;&#125; SHASecure Hash Algorithm，即安全散列算法。SHA1 是基于 MD4 算法的，现在已成为公认的最安全的散列算法之一。SHA-1 算法生成的摘要信息长度为160位，由于生成的摘要信息更长，运算的过程更加复杂，在相同的硬件上，SHA-1 的运行速度比MD5更慢，但是也更为安全。 基于 Java 的 SHA-1 算法的使用：12345public static byte[] testSHA1(String content) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("SHA-1"); byte[] bytes = md.digest(content.getBytes("utf8")); return bytes;&#125; 由于计算出的摘要转换成字符串，可能会生成一些无法显示和网络传输的控制字符，因此，需要对生成的摘要字符串进行编码，常用的编码方式包括十六进制编码与 Base64 编码。 十六进制编码基于 Java 的十六进制编码与解码的实现：1234567891011121314151617181920212223242526272829303132333435private static String bytes2hex(byte[] bytes) &#123; StringBuilder hex = new StringBuilder(); for (int i = 0; i &lt; bytes.length; i++) &#123; byte b = byte[i]; boolean negative = false; // 是否为负数 if (b &lt; 0) negative = true; int inte = Math.abs(b); if (negative) inte = inte | 0x80; // 负数会转成正数（最高位的负号变成数值计算），再转十六进制 String temp = Integer.toHexString(inte &amp; 0xFF); // &amp;0xff可以将高的24位置为0，低8位保持原样 if (temp.length() == 1) &#123; hex.append("0"); &#125; hex.append(temp.toLowerCase()); &#125; return hex.toString();&#125;private static byte[] hex2bytes(String hex) &#123; byte[] bytes = new byte[hex.length()/2]; // 8位二进制可以转换为两位16进制 for(int i = 0; i &lt; hex.length(); i = i + 2) &#123; String subStr = hex.substring(i, i + 2); boolean negative = false; // 是否为负数 int inte = Integer.parseInt(subStr, 16); if (inte &gt; 127) negative = true; if (inte == 128) &#123; int = -128; &#125; else if &#123; inte = 0 - (inte &amp; 0x7F); &#125; byte b = (byte)inte; bytes[i/2] = b; &#125; return bytes;&#125; 由于 Java 中没有无符号整型，解码时需要先将符号位进行还原，再对数值进行转换。 Base64 编码是一个基于 64 个可打印字符来表示二进制数据的方法，每6位为一个单元。Base64编码要求把3个8位字节（38=24）转化为4个6位的字节（46=24），之后在6位的前面补两个0，形成8位一个字节的形式。Base64不是一种加密算法，仅仅是一种编码算法而已。它可以将一组二进制信息编码成可打印的字符，在网络上传输与展现。 基于 Java 的 Base64 算法的使用：123456789private static String byte2base64(byte[] bytes) &#123; BASE64Encoder base64Encoder = new BASE64Encoder(); return base64Encoder.encode(bytes);&#125;private static byte[] base642byte(String base64) throws IOException &#123; BASE64Decoder base64Decoder = new BASE64Decoder(); return base64Decoder.decodeBuffer(base64);&#125; 彩虹表破解 Hash 算法Rainbow Table 法是一种破解哈希算法的技术，从原理上来说能够对任何一种 Hash 算法进行攻击。采用穷举法，理论上彩虹的大小是可以无穷大的。 对称加密算法加密和解密方事先都必须知道加密的密钥。 对称加密算法的特点是算法公开、计算量小、加密速度快、加密速率高。优势在于加解密的高速度和使用长密钥时的难破解性，但是，对称加密算法的安全性依赖于密钥，泄露密钥就意味着任何人都可以对加密的密文进行解密。 DES 算法DES 算法属于对称加密算法，明文按 64 位进行分组，密钥长 64 位，但事实上只有 56 位参与DES运算(8的倍数位为校验位)，分组后的明文和56位的密钥按位替代或交换的方法形成密文。3DES 是DES向AES过渡的加密算法，它使用3条56位的密钥对数据进行3次加密，是DES的一个更安全的变形。基于 Java 的 DES 算法的使用：123456789101112131415161718192021222324252627282930313233/*** 生成 DES 密钥*/public static String genKeyDES() throws Exception &#123; KeyGenerator keyGen = keyGenerator.getInstance("DES"); keyGen.init(56); SecretKey key = keyGen.generateKey(); String base64Str = byte2base64(key.getEncoded()); return base64Str;&#125;public static SecretKey loadKeyDES(String base64Key) throws Exception &#123; byte[] bytes = base642byte(base64Key); SecretKey key = new SecretKeySpec(bytes, "DES"); return key;&#125;/*** 加密与解密*/public static byte[] encryptDES(byte[] source, SecretKey key) throws Exception &#123; Cipher cipher = Cipher.getInstance("DES"); cipher.init(Cipher.ENCRYPT_MODE, key); byte[] bytes = cipher.doFinal(source); return bytes;&#125;public static byte[] decryptDES(byte[] source, SecretKey key) throws Exception &#123; Cipher cipher = Cipher.getInstance("DES"); cipher.init(Cipher.DECRYPT_MODE, key); byte[] bytes = cipher.doFinal(source); return bytes;&#125; AES 算法Advanced Encryption Standard，即高级加密标准。AES 算法作为新一代的数据加密标准，汇聚了强安全性、高性能、高效率、易用和灵活等优点，设计有三个密钥长度(128，192，256位)，比 DES 算法的加密强度更高，更为安全。基于 Java 的AES 算法的使用。123456789101112131415161718192021222324252627282930313233/*** 生成 AES 密钥*/public static String genKeyAES() throws Exception &#123; KeyGenerator keyGen = keyGenerator.getInstance("AES"); keyGen.init(128); SecretKey key = keyGen.generateKey(); String base64Str = byte2base64(key.getEncoded()); return base64Str;&#125;public static SecretKey loadKeyAES(String base64Key) throws Exception &#123; byte[] bytes = base642byte(base64Key); SecretKey key = new SecretKeySpec(bytes, "AES"); return key;&#125;/*** 加密与解密*/public static byte[] encryptAES(byte[] source, SecretKey key) throws Exception &#123; Cipher cipher = Cipher.getInstance("AES"); cipher.init(Cipher.ENCRYPT_MODE, key); byte[] bytes = cipher.doFinal(source); return bytes;&#125;public static byte[] decryptDES(byte[] source, SecretKey key) throws Exception &#123; Cipher cipher = Cipher.getInstance("AES"); cipher.init(Cipher.DECRYPT_MODE, key); byte[] bytes = cipher.doFinal(source); return bytes;&#125; 由于美国对于加密软件出口的控制，如果使用192位和256位的密钥，则需要另外下载无政策和司法限制的文件，否则程序运行时会出现异常。 非对称加密算法如果使用公钥对数据进行加密，只有用对应的私钥才能进行解密，而如果使用私钥对数据进行加密，那么只有用对应的公钥才能进行解密。因为加密和解密使用的是两个不同的密钥，所以这种算法称为非对称加密算法。 RSA 算法 RSA 是目前最有影响力的非对称加密算法，它能够抵抗到目前为止已知的所有密码攻击，已被 ISO 推荐为公钥数据加密标准。RSA 算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但反过来对其乘积进行因式分解却及其困难，因此可以将乘积公开作为加密密钥。基于Java 的 RSA 算法的使用：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/*** 生成公钥与私钥*/public static KeyPair getKeyPair() throws Exception &#123; KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance("RSA"); keyPairGenerator.initialize(512); KeyPair keyPair = keyPairGenerator.generateKeyPair(); return keyPair;&#125;public static String getPublicKey(KeyPair keyPair) &#123; PrivateKey publicKey = KeyPair.getPublic(); byte[] bytes = publicKey.getEncoded(); return byte2base64(bytes);&#125;public static String getPrivateKey(KeyPair keyPair) &#123; PrivateKey privateKey = KeyPair.getPrivate(); byte[] bytes = privateKey.getEncoded(); return byte2base64(bytes);&#125;/*** 将 String 类型的密钥转换为 PublicKey 和 PrivateKey 对象*/public static PublicKey string2PublicKey(String pubStr) throws Exception &#123; byte[] keyBytes = base642byte(pubStr); X509EncodedKeySpec keySpec = new X509EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance("RSA"); PublicKey publickey = KeyFactory.generatePublic(keySpec); return publicKey;&#125;public static PrivateKey string2PrivateKey(String priStr) throws Exception &#123; byte[] keyBytes = base642byte(priStr); X509EncodedKeySpec keySpec = new X509EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance("RSA"); PrivateKey privatekey = KeyFactory.generatePrivate(keySpec); return privatekey;&#125;/*** 使用公钥加密，私钥解密*/public static byte[] publicEncrypt(byte[] content, publicKey) throws Exception&#123; Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.ENCRYPT_MODE, publicKey); byte[] bytes = cipher.doFinal(content); return bytes;&#125;public static byte[] privateEncrypt(byte[] content, privateKey) throws Exception&#123; Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.DECRYPT_MODE, privateKey); byte[] bytes = cipher.doFinal(content); return bytes;&#125; 数字签名 签名认证是非对称加密技术与数字摘要技术的综合运用，指的是将通信内容的摘要信息使用发送者的私钥进行加密，然后将密文与原文一起传输给信息的接收者，接受者通过发送者的公钥解密被加密的摘要信息，然后使用与发送者相同的摘要算法，对接收到的内容采用相同的方式产生摘要串，与解密的摘要串进行对比，如果相同，则说明接收到的内容是完整的，在传输过程中没有受到第三方篡改，否则说明通信内容已被第三方修改。 MD5withRSA算法实现：123456789101112131415161718192021private static byte[] sign(byte[] content, PrivateKey privateKey) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("MD5"); byte[] bytes = md.digest(content); Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.ENCRYPT_MODE, privateKey); byte[] encryptBytes = cipher.doFinal(bytes); return encryptBytes;&#125;private static boolean verify(byte[] content, byte[] sign, PublicKey publicKey) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("MD5"); byte[] bytes = md.digest(content); Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.DECRYPT_MODE, publicKey); byte[] decryptBytes = cipher.doFinal(sign); if (byte2base64(decryptBytes).equals(byte2base64(bytes))) &#123; return true; &#125; else &#123; return false; &#125;&#125; 基于 Java 的 Signature API 的使用：12345678910111213private static byte[] sign(byte[] content, PrivateKey privateKey) throws Exception &#123; Signature signature = Signature.getInstance("MD5withRSA"); signature.initSign(privateKey); signature.update(content); return signature.sign();&#125;private static boolean verify(byte[] content, byte[] sign, PublicKey publicKey) throws Exception &#123; Signature signature = Signature.getInstance("MD5withRSA"); signature.initSign(publicKey); signature.update(content); return signature.verify(sign);&#125; SHA1withRSA算法实现：123456789101112131415161718192021private static byte[] sign(byte[] content, PrivateKey privateKey) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("SHA1"); byte[] bytes = md.digest(content); Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.ENCRYPT_MODE, privateKey); byte[] encryptBytes = cipher.doFinal(bytes); return encryptBytes;&#125;private static boolean verify(byte[] content, byte[] sign, PublicKey publicKey) throws Exception &#123; MessageDigest md = MessageDigest.getInstance("SHA1"); byte[] bytes = md.digest(content); Cipher cipher = Cipher.getInstance("RSA"); cipher.init(Cipher.DECRYPT_MODE, publicKey); byte[] decryptBytes = cipher.doFinal(sign); if (byte2base64(decryptBytes).equals(byte2base64(bytes))) &#123; return true; &#125; else &#123; return false; &#125;&#125; 基于 Java 的 Signature API 的使用： 12345678910111213private static byte[] sign(byte[] content, PrivateKey privateKey) throws Exception &#123; Signature signature = Signature.getInstance("SHA1withRSA"); signature.initSign(privateKey); signature.update(content); return signature.sign();&#125;private static boolean verify(byte[] content, byte[] sign, PublicKey publicKey) throws Exception &#123; Signature signature = Signature.getInstance("SHA1withRSA"); signature.initSign(publicKey); signature.update(content); return signature.verify(sign);&#125; 数字证书Digital Certificate 也称电子证书。集合了多种密码学的加密算法，证书自身带有公钥信息，可以完成相应的加密，解密操作，同时，还拥有自身信息的数字签名，可以鉴别证书的颁发机构，以及证书内容的完整性。由于证书本身含有用户的认证信息，因此可以作为用户身份识别的依据。内容： 对象的名称(人，组织，服务器) 证书的过期时间 证书的颁发机构(谁为证书担保) 证书颁发机构对证书信息的数字签名 签名算法 对象的公钥 数字证书一般采用 Base64 编码后再进行存储 X.509提供了一种标准的方式，将证书信息规范地存储到一系列可解析的字段当中。 证书签发需要由数字证书认证机构(Certificate Authority，CA)来进行颁发，只有经过 CA 颁发的数字证书在网络中才具备可认证性。 数字证书的签发过程实际上就是对数字证书的内容，包括证书代表对象的公钥进行数字签名，而验证证书的过程，实际上是验证证书的数字签名，包含了对证书有效期的验证。 证书检验客户端接收到数字证书时，首先会检查证书的认证机构，如果该机构是权威的证书认证机构，则通过该权威认证机构的根证书获得证书颁发者的公钥，通过该公钥，对证书的数字签名进行检验，并验证证书的有效时间是否过期。 根证书是证书认证机构给自己颁发的数字证书，是证书信任链的起始点，安装根证书则意味着对这个证书认证机构的信任。证书链的终点就是根证书。 证书管理任何机构或者个人都可以申请数字证书。 keytool是 Java 的数字证书管理工具，用于数字证书的生成、导入、导出与撤销等操作。 构建自签名证书 证书导出 导出CSR 导入数字证书OpenSSLOpenSSL 包含一个开源的 SSL 协议的实现，虽然 OpenSSL 使用 SSL 作为其名字的重要组成部分，到实现的功能却远远超出了 SSL 协议本身。包含三个组成部分： SSL协议库 密码算法库 各种与之相关的应用程序 安装 配置 生成 OpenSSL 根证书 OpenSSL 签发服务端证书 OpenSSL 签发客户端证书 证书的使用证书除包含一些认证信息以外，还包含了证书持有人的公钥，外界获得证书以后，可以使用对相关信息进行加密，而信息接收方则使用私钥进行解密。 摘要认证 相对于基于 TCP 协议层面的通信方式，针对 HTTP 协议的攻击门槛更低，因此，基于 HTTP 协议的 Web 与 SOA 架构，在应用的安全性方面需要更加重视。 摘要认证的原理 鉴于使用 HTTPS 性能上的成本以及需要额外申请 CA 证书，在这种情况下，一般采用对参数和响应进行摘要的方法，即能够满足需求。由于传递端和接收端都认为 HTTP 协议的请求参数是无序的，因此客户端与服务端双方需要约定好参数的排序方式。请求的参数经过排序后，再将参数名称和值经过一定的策略组织起来，加上一个密钥 secret，也就是所谓的盐，然后通过约定的摘要算法生成数字摘要，传递给服务端。但是，摘要的安全性取决于secret的安全性。 摘要认证的实现客户端参数摘要生成基于 Java 客户端参数摘要生成的部分关键代码：123456789101112131415161718private String getDigest(Map&lt;String, String&gt; params) throws Exception &#123; String secret = "abcdefjhijklmn"; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext()) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; keyvalueStr += secret; String base64Str = byte2base64(getMD5(keyvalueStr)); return base64Str;&#125; 服务端参数摘要校验基于 Java 服务端参数摘要校验的部分关键代码：12345678910111213141516171819202122private boolean validate(Map params, String digest) throws Exception &#123; String secret = "abcdefjhijklmn"; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext()) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; keyvalueStr += secret; String base64Str = byte2base64(getMD5(keyvalueStr)); if(base64Str.equals(digest)) &#123; return true; &#125; else &#123; return false; &#125;&#125; 服务端响应摘要生成基于 Java 服务端响应摘要生成的部分关键代码：123456private String getDigest(String content) throws Exception &#123; String secret = "abcdefjhijklmn"; content += secret; String base64Str = byte2base64(getMD5(content)); return base64Str;&#125; 客户端响应摘要校验基于 Java 客户端响应摘要校验的部分关键代码12345678910private boolean validate(String responseContent, String digest) throws Exception &#123; String secret = "abcdefjhijklmn"; byte[] bytes = getMD5(responseContent + secret); String responseDigest = byte2base64(bytes); if (responseDigest.equals(digest)) &#123; return true; &#125; else &#123; return false; &#125;&#125; 签名认证签名认证的原理类似摘要认证，但签名认证不再加上 secret ，而是直接通过约定的摘要算法来生成数字摘要，并且使用客户端私钥对数字摘要进行加密，将加密的密文传递给服务端。 签名认证的实现客户端参数签名生成1234567891011121314151617181920212223242526272829303132333435363738394041private String getSign(Map&lt;String, String&gt; params) throws Exception &#123; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; byte[] md5Bytes = getMD5(keyvalueStr); PrivateKey privateKey = AsymmetricalUtil.string2PrivateKey(consumerPrivateKey); byte[] encryptBytes = AsymmetricalUtil.privateEncrypt(md5Bytes, privateKey); String hexStr = byte2hex(encryptBytes); return hexStr;&#125;/*** 使用 Java API 对客户端请求进行数字签名*/private String getSign(Map&lt;String, String&gt; params) throws Exception &#123; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext()) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; PrivateKey privateKey = AsymmetricalUtil.string2PrivateKey(consumerPrivateKey); Signature signature = Signature.getInstance("MD5withRSA"); signature.initSign(privateKey); signature.update(keyvalueStr.getBytes()); return byte2hex(signature.sign());&#125; 服务端参数签名校验123456789101112131415161718192021222324252627282930313233343536373839404142434445private boolean validate(Map params, String digest) throws Exception &#123; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext()) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; String hexStr = byte2hex(getMD5(keyvalueStr)); PublicKey publickey = AsymmetricalUtil.string2PublicKey(consumerPublicKey); byte[] decryptBytes = AsymmetricalUtil.publicDecrypt(hex2bytes(digest), publickey); String decryptDigest = bytes2hex(decryptBytes); if(hexStr.equals(decryptDigest)) &#123; return true; &#125; else &#123; return false; &#125;&#125;/*** 使用 Java 数字签名 API 对客户端进行数字签名校验*/private boolean validate(Map params, String sign) throws Exception &#123; Set&lt;String&gt; keySet = params.keySet(); // 使用 treeSet 排序 TreeSet&lt;String&gt; sortSet = new TreeSet&lt;String&gt;(); sortSet.addAll(keySet); String keyvalueStr = ""; Iterator&lt;String&gt; it = sortSet.iterator(); while(it.hasNext()) &#123; String key = it.next(); String value = params.get(key); keyvalueStr += key + value; &#125; PublicKey publickey = AsymmetricalUtil.string2PublicKey(consumerPublicKey); Signature signature = Signature.getInstance("MD5withRSA"); signature.initVerify(publickey); signature.update(keyvalueStr.getBytes()); return signature.verify(hex2bytes(sign));&#125; 服务端响应签名生成123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private String getSign(String content) throws Exception &#123; byte[] md5Bytes = getMD5(content); PrivateKey privatekey = AsymmetricalUtil.string2PrivateKey(providePrivateKey); byte[] encryptBytes = AsymmetricalUtil.privateEncrypt(md5Bytes, privateKey); String hexStr = bytes2hex(encryptBytes); return hexStr;&#125;/*** 使用 Java 数字签名 API 生成服务器端响应的数字签名*/private String getSign(String content) throws Exception &#123; PrivateKey privatekey = AsymmetricalUtil.string2PrivateKey(providePrivateKey); Signature signature = Signature.getInstance("MD5withRSA"); signature.initSign(privateKey); signature.update(content.getBytes()); String hexStr = bytes2hex(signature.sign()); return hexStr;&#125;``` #### 客户端响应签名校验```javaprivate boolean validate(String responseContent, String digest) throws Exception &#123; byte[] bytes = getMD5(responseContent); String responseDigest = bytes2hex(bytes); PublicKey publickey = AsymmetricalUtil.string2PublicKey(providerPublicKey); byte[] decryptBytes = AsymmetricalUtil.publicDecrypt(hex2bytes(digest), publickey); String decryptDigest = byte2hex(decryptBytes); if(responseContent.equals(decryptBytes)) &#123; return true; &#125; else &#123; return false; &#125;&#125;/*** 使用 Java 数字签名 API 对服务端响应数字签名的校验*/private boolean validate(String responseContent, String sign) throws Exception &#123; PublicKey publickey = AsymmetricalUtil.string2PrivateKey(providePublicKey); Signature signature = Signature.getInstance("MD5withRSA"); signature.initVerify(publickey); signature.update(content.getBytes()); String hexStr = bytes2hex(responseContent.getBytes()); return signature.verify(hex2bytes(sign));&#125; HTTPS 协议HTTPS 协议原理Hypertext Transfer Protocol over Secure Socket Layer，即基于 SSL 的 HTTP 协议。HTTPS 协议在 HTTP 协议与 TCP 协议增加了一层安全层，所有请求和响应数据在经过网络传输之前，都会先进行加密，然后再进行传输。HTTPS 既支持单向认证，也支持双向认证。 SSL TLSSecure Sockets Layer，即安全套接层。SSL 协议的优势在于它与应用层协议独立无关，高层的应用协议如 HTTP、SSH、FTP等等，能透明的建立于 SSL 协议之上，在应用层通信之前就已经完成加密算法、通信密钥的协商以及服务端及客户端的认证工作，在此之后所有应用层协议所传输的数据都会被加密，从而保证通信的私密性。SSL 的继任者是 TLS 协议，全称为 Transport Layer Security，即传输层安全协议，是基于 SSL 协议的通用化协议。 SSL TLS协议均可分为两层： Record Protocol 记录协议 建立在可靠的传输协议(如TCP)之上，提供数据封装、加密解密、数据压缩、数据校验等基本功能。 Handshake Protocol 握手协议 在实际的数据传输开始前，进行加密算法的协商，通过密钥的交换，通信双方身份的认证等工作。 JSSE(Java Security Socket Extension)是 sun 公司为了解决互联网信息安全传输提出的一个解决方案，它实现了 SSL 和 TSL 协议，包含了数据加密、服务器验证、消息完整性和客户端验证等技术。 首先，将生成的客户端私钥和数字证书进行导出，生成 Java 环境可用的 keystore 文件。Java 实现的通信客户端SSLSocket 初始化：1234567891011121314151617181920212223242526272829303132333435public static void init() throws Exception &#123; String host = "127.0.0.1"; int port = 1234; // 包含客户端的私钥与服务端的证书 String keystorePath = "/home/user/temp/testssl/client.keystore"; String trustKeystorePath = "home/user/temp/testssl/ca-trust.keystore"; String keystorePassword = "123456"; SSLContext sslContext = sslContext.getInstance("SSL"); // 密钥库 KeyManagerFactory kmf = KeyManagerFactory.getInstance("sunx509"); // 信任库 TrustManagerFactory tmf = TrustManagerFactory.getInstance("sunx509"); KeyStore keyStore = KeyStore.getInstance("pkcs12"); KeyStore trustKeystore = KeyStore.getInstance("jks"); FileInputStream keystoreFis = new FileInputStream(keystorePath); keystore.load(keystoreFis, keystorePassword.toCharArray); FileInputStream trustKeystoreFis = new FileInputStream(trustKeystorePath); trustKeystore.load(trustKeystoreFis, keystorePassword.toCharArray()); kmf.init(keystore, keystorePassword.toCharArray()); tmf.init(trustKeystore); // 上下文初始化 sslContext.init(kmf.getKeyManagers(), tmf.getTrustManagers(), null); // SSLSocket 初始化 sslSocket = (SSLSocket) sslContext.getSocketFactory().createSocket(host, port);&#125; 进行 SSL 通信：123456789101112public static void process() throws Exception &#123; String hello = "hello yeonsea!"; OutputStream output = sslSocket.getOutputStream(); output.write(hello.getBytes(), 0, hello.getBytes().length); output.flush(); byte[] inputBytes = new byte[20]; InputStream input = sslSocket.getInputStream(); input.read(inputBytes); System.out.println(new String(inputBytes));&#125; Java 实现的通信服务端SSLServerSocket 初始化：123456789101112131415161718192021222324252627282930313233public static void init() throws Exception &#123; int port = 1234; // keystore 中包含服务端的私钥与服务端的证书 String keystorePath = "/home/user/temp/testssl/server.keystore"; String trustKeystorePath = "/home/user/temp/testssl/ca-trust.keystore"; String keystorePassword = "123456"; SSLContext sslContext = SSLContext.getInstance("SSL"); // 密钥库 KeyManagerFactory kmf = KeyManagerFactory.getInstance("sunx509"); // 信任库 TrustManagerFactory tmf = TrustManagerFactory.getInstance("sunx509"); KeyStore keyStore = KeyStore.getInstance("pkcs12"); KeyStore trustKeystore = KeyStore.getInstance("jks"); FileInputStream keystoreFis = new FileInputStream(keystorePath); keystore.load(keystoreFis, keystorePassword.toCharArray); FileInputStream trustKeystoreFis = new FileInputStream(trustKeystorePath); trustKeystore.load(trustKeystoreFis, keystorePassword.toCharArray()); kmf.init(keystore, keystorePassword.toCharArray()); tmf.init(trustKeystore); // 上下文初始化 sslContext.init(kmf.getKeyManagers(), tmf.getTrustManagers(), null); serverSocket =(SSLServerSocket).getSocketFactory().createServerSocket(port); serverSocket.setNeedClientAuth(true);&#125; 处理 SSL 请求：12345678910111213141516public static void process() throws Exception &#123; String bye = "bye bye"; while(true) &#123; Socket socket = serverSocket.accept(); byte[] inputBytes = new byte[20]; InputStream input = socket.getInputStream(); input.read(inputBytes); System.out.println(new String(inputBytes)); OutputStream output = socket.getOutputStream(); output.write(bye.getBytes, 0, bye.getBytes().length); output.flush; &#125;&#125; 部署 HTTPS Web以 Tomcat 为例 Tomcat 单向认证配置修改 Tomcat 配置：1234567&lt;!-- cd tomcat/conf; vim server.xml; 找到默认注释的一段 --&gt;&lt;Connector port="443" protocol="HTTP/1.1" SSLEnabled="true" maxThreads="150" scheme="https" secure="true" clientAuth="false" sslProtocol="TLS" keystoreFile="home/user/temp/testssl/server.keystore" keystorePass="123456" keystoreType="pkcs12" /&gt; 安装 authbind配置443端口重启 tomcat Tomcat 双向认证的配置123456789&lt;Connector port="443" protocol="HTTP/1.1" SSLEnabled="true" maxThreads="150" scheme="https" secure="true" clientAuth="false" sslProtocol="TLS" keystoreFile="home/user/temp/testssl/server.keystore" keystorePass="123456" keystoreType="pkcs12" truststoreFile="home/user/temp/testssl/ca-trust.keystore" truststorePass="123456" truststoreType="jks" /&gt; OAuth 协议 随着互联网的深入发展，一些互联网巨头逐渐累积了海量的用户和数据。对于平台级的软件厂商来说，用户需求多种多样，变化万千，以一己之力予以充分满足，难免疲于奔命，因此，将数据以接口的形式下放给众多的第三方开发者，便成了必然趋势。第三方开发者经过二次开发，满足一小部分的用户的独特需求，即能够使自己获得利益，也能够让数据流动起来，在大平台周围形成一个良性的生态环境，最终达到用户、平台商、第三方开发者共赢。 平台商必须保障对于用户私有数据的访问，均是经过用户授权的合法行为，且不会对第三方泄露类似用户名和密码这样的核心数据。 OAuth 的介绍旨在为用户资源的授权访问提供一个安全、开放的标准。 Oauth 授权过程要获得 OAuth 协议授权，首先需要第三方开发者向平台商申请应用 ID，即 appId，对自己的 APP 进行注册。一次 OAuth 授权涵盖了三个角色：普通用户(consumer)、第三方应用(ISV)、平台商(platform)。 OAuth授权是一个相对较复杂的体系，涵盖系统设计的方方面面，不仅包括之前所说的认证过程，还需要解决开发者入驻、权限粒度的控制、token 生成和校验、分布式 Session、公私钥管理等一系列问题。]]></content>
      <tags>
        <tag>Structure Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch概览]]></title>
    <url>%2F2019%2F09%2F10%2FElasticsearch%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[入门全文搜索引擎基于 Lucene 构建的开源、分布式、RESTful接口 ^1 Github 的搜索是基于 Elasticsearch 构建的，只索引项目主分支，但包括20亿个索引文档，30TB的索引文件 Shay Banon 失业开发者 给妻子构建食谱搜索引擎 早期版本的 Lucene 为方便Java可以在应用中增加搜索功能 第一个开源项目“Compass” 重写 Compass 使其成为一个独立的项目，Elasticsearch 横向可扩展 分片机制提供更好的分布性：sharding，类似 HDFS 的块机制 高可用：replica 使用简单 Lucene：最初由 Doug Cutting 开发 倒排索引 基础知识术语概念 索引词：term 文本：text 分析：Analysis 集群：Cluster 节点：node 路由：routing 分片：shard 主分片：primary shard 副本分片：replica shard 副本：replica 索引：index 索引是具有相同结构的文档集合 类型：type 类型是索引的逻辑分区 文档：document 存储的一个 JSON 格式的字符串 映射：mapping 字段：field 开源字段：source field 主键：ID 每个 Elasticsearch 分片是一个 Lucene 的索引，有文档存储数量限制 JSON 轻量级数据交换格式 对外接口curlREST：REpresentational State Transfer 表述性状态传输 一种约定 简化沟通成本 索引索引管理 创建 删除 获取 打开 关闭 索引映射管理 增加映射 获取索引 索引别名 过滤索引别名 删除别名 查询现有的别名 索引配置 更新索引配置 获取配置 索引分析 索引模板 索引模板就是创建好一个索引参数设置 settings 和 映射 mapping 的模板，在创建新索引的时候指定模板名称就可以使用模板定义好的参数设置和映射 创建 删除 获取 复制配置 重建索引 索引监控 索引统计 索引分片 索引恢复 索引分片存储 状态管理 清除缓存 索引刷新 冲洗 合并索引 文档管理 增加文档 更新删除 查询 多文档操作 索引词频率 查询更新接口 映射映射是定义存储和索引文档以及字段的过程 字段数据类型 元字段 映射参数 动态映射：在 Elasticsearch 中可以不事先建好索引结构，在使用的时候可以直接插入文档到索引。 搜索深入搜索搜索方式 URL 搜索 GET方式 POST 请求参数搜索 排序 sort 数据列过滤 脚本支持重新评分对短语进行重新评分，然后再查询滚动查询请求Elasticsearch 提供了滚动 API 来解决此问题，这有点像数据库中的游标隐藏内容查询inner hits 嵌套结构相关搜索函数 Preference 搜索分片副本执行偏好设置 索引加权 index_boost 最小分值 min_score 分值解释 explain 可以使每个命中的查询解释它的得分是如何计算出来的 分片情况查询 _search_shards 总分查询 _count 是否存在查询 验证接口搜索模板查询DSLDomain-specific LanguageElasticsearch 提供了基于 JSON 的完整查询 DSL 来定义查询全文搜索 布尔查询 短语查询 短语前缀查询 多字段查询 Lucene 语法查询 简化查询字段查询 单字段查询 多字段查询 范围查询复合查询 常数得分查询 布尔查询 最大值获取查询 boosting 查询 指定索引查询 过滤查询 限制查询- 连接查询 嵌套查询 父子文档查询地理查询Elasticsearch 支持两种地理数据类型的字段：地理点类型，支持经度纬度对；地理形状类型，支持点、线、圈、多边形、多边形集合等。 地理形状查询 地理范围查询 地理距离查询 地理距离范围查询 多边形地理查询 地理散列单元查询跨度查询 索引词跨度查询 多索引词跨度查询 首跨度查询 接近跨度查询 或跨度查询 非跨度查询 包含跨度查询 内部跨度查询高亮显示Elasticsearch 中的高亮显示是来源于 Lucene 的功能，允许一个或者多个字段上突出显示搜索内容简化查询catAPI常用简化查询指标 indices nodes recovery thread pool 聚合聚合分类度量聚合 平均值聚合 基数聚合 最大值聚合 最小值聚合 和聚合 值基数聚合 统计聚合 百分比聚合 百分比分级聚合 最高命中排行聚合 脚本度量聚合 地理边界聚合 地理重心聚合 分组聚合 子聚合 直方图聚合 日期直方图聚合 时间范围聚合 范围聚合 过滤聚合 多重过滤聚合 空值聚合 嵌套聚合 采样聚合 重要索引词聚合 索引词聚合 总体聚合 地理点距离聚合 地理散列网格聚合 IPv4 范围聚合管道聚合 平均分组聚合 移动平均聚合 总和分组聚合 总和累计聚合 最大分组聚合 最小分组聚合 统计分组聚合 百分位分组聚合 差值聚合 分组脚本聚合 串行差分聚合 分组选择器聚合 集群管理集群节点监控 对 Elasticsearch 监控的API 主要有三类：一类是集群相关的，以_cluster开头，第二类是监控节点相关的，以_nodes开头，第三类是任务相关的，以_tasks开头 集群健康值 集群状态 集群统计 集群任务管理 待定集群任务 节点信息 节点统计 集群分片迁移 移动 remove 取消 cancel 分配 allocate 集群配置更新集群节点配置 主节点 主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是集群的一部分，并决定哪些分片分配给相关的节点。 数据节点 主要是存储索引数据的节点，主要对文档进行增删改查、聚合操作等。 客户端节点 当主节点和数据节点配置都设置为 false 时，该节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户端节点表现为智能负载平衡器。独立的客户端节点在一个比较大的集群中是非常有用的，它协调主节点和数据节点，客户端节点加入集群可以得到集群的状态，根据集群的状态可以直接发送路由请求。 部落节点 部落节点可以跨越多个集群，它可以接收每个集群的状态，然后合并成一个全局集群的状态，它可以读写所有节点上的数据。节点发现 ping 模块 单播模块主节点选举在集群中，系统会自动通过 ping 来进行选举节点或者加入主节点，这些都是自动完成的。故障检测集群平衡配置 分片分配设置 基于磁盘的配置 分片智能分配 分片配置过滤 其他集群配置索引分词器分词器的概念分词器analyzer的作用是当一个文档被索引的时候，分词器从文档中提取出若干词元token来支持索引的存储和搜索。 分词器 是由一个分解器 tokenizer、零个或多个词元过滤器 token filters 组成。 中文分词器 smartcn IKAnanlyzer插件 java 插件 jar 站点插件 js html css 混合插件插件管理正常情况下插件位于$ES_HOME/bin 下通过rpm等安装的位置可能会不同插件安装 查询插件 删除插件 Silent/Verbose 参数 更多调试信息 自定义配置目录 超时设置 代理设置 自定义插件目录 强制插件 插件清单 API 插件 报警插件 分词插件 发现插件 管理和站点插件 高级配置 在Elasticsearch 的配置中，主要有两种配置方式，一种是静态配置，另一种是动态配置。静态配置的参数只能在配置文件中事先写好，动态配置的参数可以通过_cluster/settings进行设置。 网络相关配置 本地网关配置 HTTP 配置 网络配置 常用网络配置，高级网络配置，高级TCP配置，传输和HTTP协议 传输配置 TCP传输，本地传输，传输追踪脚本配置 脚本使用 脚本配置 索引脚本，启用动态脚本，脚本自动重载，本地java脚本，lucene表达式脚本，得分，文档字段，保存的字段，在脚本中访问文档的得分，源字段，Groovy内置方法快照和恢复配置 只读仓库 快照 恢复 快照状态线程池配置线程池类型 cached 线程池是一个无限的线程池，如果存在挂起的请求时，就会产生一个线程。这个线程池用来防止提交的请求被阻塞或丢弃。 fixed 线程池拥有固定大小的线程来操作队列中的请求（任意界限）直到请求没有线程提供服务。size参数控制线程的数量，queue_size 参数可以控制没有线程执行的请求队列的大小。默认设置为-1，意味着无限大。当请求到达而且队列已经满了，请求会被终止。 scaling 线程池拥有动态数量的线程。线程的数量与工作量成正比，并且在1和size参数值之间变化。keep_alive参数决定一个线程的空闲时间。 处理器设置处理器的数量是自动检测的，线程池的设置会基于结果自动设置。有时，处理器的数量会被错误检测，在这种情况下，处理器的数量可以使用 processes进行明确设置。 索引配置 索引模块是控制每个索引指标的模块。索引模块包括分词、分片控制和分配、字段映射、索引相似性配置、慢查询记录、文件系统配置、控制事务和刷新模块。 缓存配置 总内存控制 列数据内存控制 请求内存控制 数据缓存 节点查询缓存 索引缓冲区 分片请求缓存 索引恢复 TTL区间索引分片分配 碎片分配过滤 延迟分配 每个节点的总碎片合并 一个 Elasticsearch 分片就是一个 Lucene 索引，Lucene索引被分解为分片。分片是索引的内部存储单元，存储索引数据并且是不变的。周期性合并(merge)小的分片为更大的分片来保持索引大小在范围内。 相似模块 配置相似性 可用的相似性模块响应慢日志监控 搜索慢日志 索引慢日志存储支持存储类型 simplefs 简单文件系统类型 niofs NIO 文件系统类型 mmapfs MMap 文件系统类型在文件系统上通过映射文件到内存(mmap)存储分片索引 default_fs 默认类型是 NIO FS 和 MMapFS 的混合，对每个类型的文件选择最佳文件系统事务日志 冲洗设置 事务日志设置 告警、监控和权限管理告警Watcher 是进行警告和通知的插件，可以根据数据的变化采取行动。它的设计原理是在 Elasticsearch 中执行查询，满足条件的情况下，产生告警。 安装结构 Trigger Inputs Condition Transform Action告警输出配置告警输出可以为邮件、Webhook、Logging、HipChat、Slack、PagerDuty告警管理 列出警告 删除警告监控 Marvel 是商业监控方案，用来监控 Elasticsearch 集群历史状态的有力工具，便于性能优化以及故障诊断。监控主要分为六个层面，分别是集群层、节点层、索引层、分片层、事件层、Sense。 安装配置 监控参数配置 监控索引配置 Kibana 配置相关 Tribe 部落节点监控配置权限管理 Shield 是商业权限管理插件，它可以保护 Elasticsearch 中的数据，采用加密的通信密码，基于角色的访问控制，IP过滤和审计等。 工作原理Shield 是 Elasticsearch 的一个插件，一旦安装完成，插件将会拦截所有 API 请求，然后对请求进行认证和授权的校验。该插件同时提供 SSL 安全协议来传输网络数据，该插件提供了审计日志记录的能力，用来进行验证和审计。 用户认证 授权 节点认证和信道加密 IP 过滤 审计用户认证用户认证方式 Native 一个内置的本地认证系统，默认可用 File 一种内置的基于文件的认证系统，默认可用 LDAP 通过外部轻量级目录协议进行身份验证 AD 通过外部活动目录服务的身份验证 PKI 通过使用可信的X.509证书的认证匿名用户访问角色管理 增加角色 查看角色 删除角色 ELK 应用 Logstash 是一个灵活的开放源码的数据收集、处理、传输的工具。Logstash 可以处理日志、事件、非结构化的数据，并把它们输出出来，包括可以输出到 Elasticsearch 中。 Kibana 是一个开源的数据可视化平台，可以把数据以强大的图形化方式展示出来。从柱状图到地图等，它可以通过多个图表的组合来生成更为强大的仪表面板，帮助人们理解、分析和分享数据。 Logstash配置配置文件的结构配置文件由输入，过滤，输出三部分组成，每部分都是由插件构成的，这些插件负责处理日志的不同过程 事件相关配置 每个事件都有不同的属性，比如 apache 的访问日志，可以包括状态码、协议、路径、客户端 IP等，在 Logstash 中这些属性叫做 fields。由于它们是事件属性，所以这些配置选项只会在过滤器和输出块中工作。 插件管理插件管理器通过 bin/logstash-plugin 的脚本来管理整个插件的生命周期，通过命令行接口(CLI)调用可以安装、卸载、升级插件。 输入插件 过滤插件 输出插件 编解码插件Kibana 配置安装比较简单，也是绿色的，解压后直接运行默认端口 5601。Discover 新的搜索 保存搜索加载保存的搜索 自动刷新 查看字段数据统计Visualize视图是定制可视化报表的地方 选择一个图标类型 选择一个数据源 可视化编辑器配置 保存编辑器Dashboard仪表盘是用一组原始图标根据需要组合成一个丰富的图形报表 空仪表盘 创建仪表盘 保存仪表盘 加载仪表盘 共享仪表盘Settings 索引设置 管理字段 告警设置 Kibana 服务器配置 管理搜索，可视化仪表板Elasticsearch 5.0 的特性与改进]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于分布式系统的稳定性]]></title>
    <url>%2F2019%2F08%2F25%2F%E5%85%B3%E4%BA%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%2F</url>
    <content type="text"><![CDATA[日志分析信息 堆栈信息 访问用户 IP、请求 URL、应用响应时间 内存回收信息 自定义信息 应用 通过异常堆栈，可以定位到谁宕机了 产生问题的程序行，对异常进行修复 访问 IP 和请求参数，排查是否遭到攻击，以及攻击的形式 应用的响应时间、垃圾回收以及系统 load 来判断系统负载，是否要增加机器 线程 dump，判断是否死锁及线程阻塞的原因 应用的 GC 日志，对系统代码和 JVM 内存参数进行优化，减少 GC 次数与 stop the world 时间，优化应用响应时间 集群监控木桶原理 监控指标 load：linux 中，可以通过 top,uptime来查看系统load值，系统load定义为特定时间间隔内运行队列中的平均线程数。 cpu 利用率 磁盘剩余空间 网络 traffic 磁盘 IO 内存使用 qps query per second rt response time select/ps update/ps、delete/ps GC JVM 虚拟机开发团队一直致力于消除或者减少工作线程因内存回收而导致的停顿，用户线程的停顿时间在不断地缩短，但是仍然没办法完全消除 心跳检测 对于自治的分布式系统而言，一般都有一整套的集群心跳检测机制，能够实时地移除掉宕机的 Slave，避免路由规则将任务分配给已宕机的机器来处理。而如果是 Master 宕机，集群能够自动地进行 Master 的选举，从而避免由 Master 宕机而导致整个集群不能提供服务的情况发生，这一类系统，如 ZooKeeper，便是一个很好的典范。也有一部分系统可以通过外部干预，使备份机器 stand by，或者是双机互为备份，以实现故障切换，如 MySQL、Nginx等，以避免单点故障的发生。 具体操作： ping 应用层检测 curl 定时访问应用中预留的自检 url 业务检测 可在 response 的 header 中约定一个值，来标识返回的结果是否正常 容量评估及应用水位当前水位 = 当前总 qps / （单台机器极限 x 机器数）x 100% 流量控制流量控制实施多个维度 对系统的总并发请求数进行限制 - 可以很好地控制系统的负载，避免出现流量突增将系统压垮的情况 限制单位时间内的请求次数（如限制qps） - 限制调用频次，防止某个外部调用的流量突增影响到服务本身的稳定性 通过白名单机制来限制每一个接入系统调用的频率等 超载的部分流量 直接返回，显示系统繁忙 通过单机内存队列来进行有限的等待 通过分布式消息队列来将用户的请求异步化 服务稳定性依赖管理服务消费日志 优雅降级 对于调用超时的非核心服务，可以设定一个阈值，如果调用超时的次数超过这个阈值，便自动将该服务降级。此时服务调用者跳过对该服务的调用，并指定一个休眠的时间点，当时间点过了以后，再次对该服务进行重试，如果服务恢复，则取消降级，否则继续保持该服务的降级状态，直到所依赖的服务故障恢复。 服务分级开关 当系统负载较高，即将突破警戒水位时，如何通过实时地屏蔽一些非核心链路的调用来降低系统的负载呢？这时需要系统预先定义一些开关控制程序的服务提供策略。开关通过修改一些预先定义好的全局变量，来控制系统的关键路径和逻辑。 应急预案高并发系统设计 高并发系统与普通系统设计的区别在于，既要保障系统的可用性和可扩展性，又要兼顾数据的一致性，还要处理多线程同步的问题。任何细微问题，都有可能在高并发环境下被无限地放大，直至系统宕机 操作原子性 原子操作指的是不可分割的操作，它要么执行成功，要么执行失败，不会产生中间状态。原子操作也是一些常见的多线程程序bug的源头。并发相关的问题对于测试来说，并不是每次都能重现，因此处理起来十分棘手。JDK5.0 以后开始提供 Atomic Class，支持 CAS compare and set 等一系列原子操作，来帮助我们简化多线程程序设计。数据count统计 CountDownLatch是做减法，CyclicBarrier是做加法,Semaphor的临界资源可以反复使用 CountDownLatch不能重置计数，CycliBarrier提供的reset()方法可以重置计数，不过只能等到第一个计数结束。Semaphor可以重复使用。 CountDownLatch和CycliBarrier不能控制并发线程的数量，Semaphor可以实现控制并发线程的数量。AtomicInteger compareAndSet 方法 调用 Unsafe 对象的 native 方法 compareAndSwapInt 方法，最终通过 Atomic::com::(x, addr, e)来实现原子操作 数据库的事务操作数据库事务具有 ACID 属性导致事务失败的原因有很多： 修改不符合表的约束规则 网络异常 存储介质故障 为了实现数据库状态的恢复，DBMS 系统通常需要维护事务日志以追踪事务中所有影响数据库数据的操作，以便执行失败时进行事务的回滚。事务日志可以提高事务执行的效率，存储引擎只需要将修改行为持久到事务日志当中，便可以只对该数据在内存中的拷贝进行修改，而不需要每次修改都将数据回写到磁盘。日志写入是一小块区域的顺序IO，而数据库数据的磁盘回写则是随机IO，磁头需要不停地移动来寻找需要更新数据的位置。 多线程同步多线程同步指的是线程之间执行的顺序，多个线程并发地访问和操作同一数据，并且执行的结果与访问或者操作的次序有关。 synchronized ReentrantLock ReentrantLock 的好处是，等待是可以中断的。通过 tryLock(timeout, unit)，可以尝试获得锁，并且指定等待时间。另一个特性是可以在构造 ReentrantLock 时使用公平锁，公平锁指的是多个线程在等待同一个锁时，必须按照申请锁的先后顺序依次获得锁。synchronized 中的锁是非公平的，默认情况下 ReentrantLock 也是非公平的，但是可以在构造函数中指定使用公平锁。对于 ReentrantLock 来说，还有一个十分实用的特性，它可以同时绑定多个 condition 条件，以实现更精细化的同步控制。 数据一致性分布式系统常常通过复制数据来提高系统的可靠性和容错性，并且将数据的副本存放到不同的机器上。由于多个副本的存在，使得维护副本一致性的代价很高。因此，许多分布式系统都采用弱一致性或者最终一致性，来提高系统的性能和吞吐能力。 最终一致性是弱一致性的一种特殊形式，这种情况下系统保证用户最终能够读取到某个操作对系统的更新，“不一致性窗口”的时间依赖于网络的延迟、系统的负载和副本的个数。最终一致性举例： mysql 主从数据同步 zookeeper 的 leader election 和 atomic broadcas 系统可扩展性系统的可扩展性也成为可伸缩性，是一种对软件系统计算处理能力的评价指标。只需要增加相应的机器，便能够使性能平滑地提升。水平扩展相对于硬件的垂直扩展来说，对于软件设计的能力要求更高，系统设计更复杂，但却能够使系统处理能力几乎可以无限制扩展系统的可扩展性也会受到一些因素的制约，CAP理论指出，系统的一致性、可用性和可扩展性三个要素对于分布式系统来说，很难同时满足。因此，在系统设计时，往往得做一些取舍。 并发减库存秒杀活动杜绝网络投机者使用工具导致不公平竞争：加速验证码，复杂验证码。 数据一致性问题： 对于高并发访问的浏览型系统来说，单机数据库如不进行扩展，往往很难支撑。因此常常会采用分库技术来提高数据库的并发能力，并且通过使用分布式缓存技术，将磁盘磁头的机械运动化为内存的高低电平，以降低数据库的压力，加快后端的响应速度。响应的越快，线程释放的也越快，能够支持的单位时间内的查询数qps也越高，并发处理能力就越强。带来的问题是跨数据库或者是分布式缓存与数据库之间难以进行事务操作。为了避免数据不一致的情况发生，并且保证前端页面能够在高并发情况下正常浏览，可以采用实际库存和浏览库存分离的方式。mysql 中 myisam 是表锁策略，innodb 是行锁策略，innodb 更适合高并发写入的场景 一个线程获得行锁以后，其他并发线程就需要等待它处理完成，这样系统将无法利用多线程并发执行的优势，并且随着并发数的增加，等待的线程会越来越多，rt 急剧飙升，最终导致可用连接数被占满，数据库拒绝服务。 可以通过将一行库存拆分成多行，便可以解除行锁导致的并发资源利用的问题。路由策略：id取模，随机。 性能优化如何寻找性能的瓶颈 Web 性能优化涉及前端优化、服务端优化、操作系统优化、数据库查询优化、JVM调优等众多领域的知识寻找可优化的点是第一步也是最重要的一步，也就是所谓的性能瓶颈，性能瓶颈实际上就是木桶原理中最短的那一块木板 前端优化工具 YSlow页面响应时间方法响应时间Java 环境下有一个十分有效的动态跟踪工具，btrace GC日志分析 GC 日志能够反应出 Java 应用执行内存回收详细情况，如 Minor GC 的频繁程度、Full GC 的频繁程度、GC 所导致应用停止响应的时间、引起 GC 的原因等。在 JVM 启动时加上几个参数1-verbose:gc -Xloggc:/gc.log -XX:+PrintGCDetails -XX:+PrintGCDataStamps 分别表示日志存放位置，输出 GC 详情，输出 GC 时间戳 CMS 收集器是一款以获取最短回收停顿时间为目的的收集器，它是基于标记清除算法实现的，整个过程大致分为四个步骤： CMS initial mark, CMS concurrent mark, CMS remark, CMS concurrent sweep 数据库查询low_slow_querylow_query_time通过 MySQL 的配置文件 my.cnf ，可以修改慢查询日志的相关配置 系统资源使用性能测试工具性能测试指的是通过一些自动化的测试工具模拟多种正常、峰值，以及异常负载对系统的各项性能指标进行测试。 ab全称为 ApacheBench，专门针对 HTTP 服务器进行性能测试的小工具，可以模拟多个并发请求来对服务器进行压力测试，得出服务器在高负载下能够支持的qps及应用的响应时间，为系统设计者提供参考依据。 Apache JMeter开源性能测试工具，比 ab 更为强大，采用纯 Java 实现，支持多种协议的性能基准测试，如HTTP,SOAP,FTP,TCP,SMTP,POP3等。提供了图形化界面 Tomcat 在启动脚本中加入如下配置，便能通过jconsole,VisualVM等工具查看系统相关信息1CATALINA\_OPTS=&quot;$CATALINA_OPTS -Djava.rmi.server.hostname=***.***.***.*** -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=**** -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot; HP LoadRunner商业付费，更成熟，更强大，支持的协议更为广泛，用户体验更高。 反向代理引流 在分布式环境下，流量真正到达服务器之前，一般会经过负载均衡设备进行转发，通过修改负载均衡的策略，可以改变后端服务器所承受的压力。 Nginx 配置权重，新版本灰度发布。 TCPCopy网易技术部2011年9月开源的一个项目，请求复制工具，能够将在线请求复制到测试机器，模拟真实环境，达到程序在不上线的情况下承担线上真实流量的效果。 性能优化措施前端性能优化 页面的 HTTP 请求数量 新建一个到服务器的HTTP连接需要重新经历TCP协议握手建立连接状态等过程，并且大部分请求和响应都包含了很多相同 header 与 cookie 内容，增加了网络带宽消耗。因此，减少 HTTP 请求的数量能够加速页面的加载，在不改变页面外观的情况下，可以通过采取合并样式和脚本文件等措施，来减少页面加载所需要请求数。 是否使用 CDN 网络 CDN 网络使得用户能够就近取得所需要的资源，降低静态资源传输的网络延迟。可以将图片、样式文件、脚本文件、页面框架等不需要频繁变动的内容推送到 CDN网络，可以提高页面加载的速度。 是否使用压缩 对于前端样式文件与脚本文件，可以将其中空格、注释等不必要的字符去掉，并且通过使用 gzip 压缩来减少网络上传输的字节数。当然，压缩也是有成本的，它会消耗一定的 CPU 资源，但通常情况下来说这种开销都是值得的。 Java 程序优化单例对于 IO 处理、数据库连接、配置文件解析加载等一些非常耗费系统资源的操作，我们必须对这些实例的创建进行限制，或者始终使用一个公用的实例，以节约系统开销，这种情况下就需要用到单例模式。 Future 模式假设一个任务执行起来需要花费一些时间，为了省去不必要的等待时间，可以先获取一个提货单，即 Future ，然后继续处理别的任务，直到货物到达，即任务执行完得到结果，此时便可以用提货单进行提货，即通过 Future 对象得到返回值。1234567891011121314151617public class TestFuture &#123; static class Job&lt;Object&gt; implements Callable&lt;Object&gt; &#123; @Override public Object call() throws Exception &#123; return loadData(); &#125; &#125; public static void main(String[] args) throws Exception &#123; FutureTask future = new FutureTask(new Job&lt;Object&gt;()); new Thread(future).start(); // do something else Object result = (Object) future.get(); &#125;&#125; FutureTask 类实现了 Future 和 Runnable 接口，FutureTask 开始后，loadData()执行时间可能较长，因此可以先处理其他事情，等其他事情处理好以后，再通过 future.get() 来获取结果，如果 loadData() 还未执行完毕，则此线程会阻塞等待。 线程池使用线程池将互不依赖的几个动作切分，通过多线程对串行工作进行改进，将成倍地提高工作效率。12345678910111213141516171819public class TestExecutorService &#123; static class Job implements Runnable &#123; @Override public void run() &#123; doWork(); // 具体工作 &#125; public void doWork() &#123; System.out.println(&quot;doing...&quot;); &#125; &#125; public static void main(String[] args) &#123; ExecutorService exec = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 10; i ++) &#123; exec.execute(new Job()); &#125; &#125;&#125; 选择就绪 JDK自1.4起开始提供全新的 IO 编程类库，简称 NIO，其不但引入了全新高效的 Buffer 和 Channel ，同时还引入了基于 Selector 的非阻塞 IO 机制，将多个异步的 IO 操作集中到一个或几个线程当中进行处理。使用 NIO 代替阻塞 IO 能提高程序的并发吞吐能力，降低系统的开销。 对于每一个请求，单独开一个线程进行响应的逻辑处理，如果客户端的数据传递并不是一直进行，而是断断续续的，则相应线程需要 IO 等待，并进行上下文切换。 Selector 机制使得线程不必等待客户端的 IO 就绪，当客户端还没就绪时，可以处理其他请求，提高了服务器的并发吞吐能力，降低了资源消耗。 减少上下文切换进程上下文切换会有一定的调度开销，这个过程中操作系统和JVM会消耗一定的CPU周期，并且由于CPU处理器会缓存一部分数据，当新线程被切换进来时，它所需要的数据可能不在CPU缓存中，因此还会导致CPU缓存的命中率下降。 降低锁竞争降低锁竞争的一种有效的方式是尽可能地缩短锁持有的时间，比如可以将一部分与锁无关的代码移出同步代码块，特别是执行起来开销较大的操作，以及可能使当前线程被阻塞的操作。另一种减小锁持有时间的方式是减小锁的粒度，将原先使用单独锁来保护的多个变量变为采用多个相互独立的锁分别进行保护，这样就能够降低线程请求锁的几率，从而减少竞争发生的可能性。当然，使用的锁越多，发生死锁的风险也就越高。第三种降低锁竞争的方式就是放弃使用独占锁，而使用其他更友好的并发方式来保障数据的同步，原子变量就是使用读写锁。对于多读少写的情况，使用读写锁能够比使用独占锁提供更高的并发数量。 压缩在进行数据传输之前，可以先将数据进行压缩，以减少网络传输的字节数，提升数据传输的速度。接收端可以将数据进行解压，以还原出传递的数据，并且经过压缩的数据还可以节约所耗费的存储介质(磁盘或内存)的空间与网络带宽，降低成本。当然，压缩需要大量的CPU计算，并且根据压缩算法的不同，计算的复杂度和数据的压缩比也存在较大差异。 结果缓存本地缓存，分布式缓存 数据库查询性能优化以mysql为例 合理使用索引 对于使用B树或B+树存储的组合索引来说，有一个最基本的原则，即“最左前缀”的原则，如果查询不是按照索引的最左列来开始查询，则无法使用到组合索引。 反范式设计范式设计好处: 冗余数据的减少，无疑节约了存储空间，而且保证了关系的一致性; 由于冗余数据的减少，当数据需要进行更新时，要修改的数据则变少了，这样会提升更新操作的速度; 范式化的表通常更小，可以更好地利用表的查询缓存来提高查询速度。但是，对于大多数复杂的业务场景来说，数据表现的纬度不可能是单表的。因此在进行查询操作时，需要进行表的关联。这不仅代价高昂，由于查询条件指定的列可能并不在同一个表中，因此也无法使用到索引，这将导致数据库的性能严重下降。 为了尽可能地避免关联查询带来的性能损耗，有人提出了反范式设计，即将一些常用的需要关联查询的列进行冗余存储，以便减少表关联带来的随机IO和全盘扫描。 使用查询缓存 query_cache_type query_cache_size query_cache_limit 使用搜索引擎 在分布式环境下，为了便于数据库扩展，提高并发处理能力，相关联的表可能并不在同一个数据库当中，而是分布在多个库当中，并且表也可能已经进行了切分，无法进行复杂的条件查询。这时候就需要搭建搜索引擎，将需要进行查询和展现的列通过一定的规则都建到索引当中，以提供复杂的垮表查询与分组操作。 使用 key-value 数据库 对于保有海量数据的互联网企业来说，多表的关联查询是非常忌讳的。出于性能的考虑，更多时候往往根据表的主键来进行查询，或者进行简单的条件查询。因此，SQL的功能被很大程度地弱化了。 GC 优化Parallel Scavenge 垃圾收集器是悲观策略，每次晋升到 OldGen 的平均大小如果大于当前OldGen的剩余空间，则触发一次FullGC。如果频繁发生，可以通过-Xmx与-Xms参数来调整整个堆的大小，以增加OldGen的大小，YoungGen对应的-Xmn保持不变。默认情况下，CMS收集器的垃圾回收会在OldGen使用了68%空间时被激活，可以调大。但如果预留的内存无法满足程序需要，则会出现 concurrent mode failure。 堆设置： Xms 是指程序启动时初始内存大小（此值可以设置成与-Xmx相同，以避免每次GC完成后 JVM 内存重新分配）。 Xmx 指程序运行时最大可用内存大小，程序运行中内存大于这个值会 OutOfMemory。 Xmn 年轻代大小（整个JVM内存大小 = 年轻代 + 年老代 + 永久代）。 XX:NewRatio 年轻代与年老代的大小比例，-XX:NewRatio=4 设置为4，则年轻代与年老代所占比值为1：4。 XX:SurvivorRatio 年轻代中Eden区与Survivor区的大小比值，-XX:SurvivorRatio=4，设置为4，则两个Survivor区与一个Eden区的比值为 2:4 XX:MaxPermSize 设置永久代大小。 XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 Xss 设置每个线程的堆栈大小。 硬件提升性能内存越大越好，硬盘读写，避免网卡成为系统吞吐的瓶颈，CPU核数。 Java 应用故障排查常用工具 jps 类似 linux 的 ps jstat 对虚拟机各种运营状态进行监控的工具，通过它可以查看虚拟机的类加载与卸载情况，管理内存使用和垃圾收集等信息，监视JIT即时编译器的运行情况等。 jinfo 用于查看应用程序的配置参数，以及打印运行JVM时所指定的JVM参数。 jstack 用来生成虚拟机当前的线程快照信息，线程快照就是当前虚拟机每一个线程正在执行的方法堆栈的集合。 jmap 查看等待回收对象的队列，查看堆的概要信息，包括采用的是哪种GC收集器，堆空间的使用情况，以及通过JVM参数指定的各个内存空间的大小。 BTrace 是一个开源的 Java 程序动态跟踪工具。基本原理是通过Hotspot虚拟机的HotSwap技术将跟踪的代码动态替换到被跟踪的Java程序内，以观察程序运行的细节。通过BTrace脚本，可以在方法执行时，输出传递给方法的参数与方法的返回值。 JConsole 是一款JDK内置的图形化性能分析工具，它可以用来连接本地或者远程正在运行的JVM，对运行的Java应用程序的性能及资源消耗情况进行分析和监控，并提供可视化的图表对相关数据进行展现。 Memory Analyzer Eclipse 插件 VisualVM 功能强大的 all-in-one 工具]]></content>
      <tags>
        <tag>调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Writing & Publishing your First NPM Package]]></title>
    <url>%2F2019%2F08%2F23%2FWriting%26Publishing_your_First_NPM_Package%2F</url>
    <content type="text"><![CDATA[Just do itNPM account Create an NPM account if you don’t yet have onenpm js. Open terminal and … 1npm adduser ** If you use taobao mirror, it will show 403 Forbidden. Then …123npm config get registry # show the current login sourcenpm config set registry=http://registry.npmjs.org # change to the npmjsnpm login # relogin Create a component1module.exports.*** = *** Create a new repository on GitHubInit and publishUnder the component, open the terminal init1234567code .git initgit add .git commit -m &quot;first commit&quot;git remote add origin https://github.com/***/***.gitgit push -u origin mastersudo git push -u origin master publish1npm publish Use it!create a new projectimport12npm i ** --saveparser index.html]]></content>
      <tags>
        <tag>Front-end</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西尔维斯特-矩阵]]></title>
    <url>%2F2019%2F08%2F19%2F%E8%A5%BF%E5%B0%94%E7%BB%B4%E6%96%AF%E7%89%B9-%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[西尔维斯特（James Joseph Sylvester, 1814~1897） 1数量可以用二维数据表格来展示 打猎得到的各种猎物和税金之间的关系可以用两个二维数据表格来表示 00 兔子 鹿 野猪 西尔维斯特 5只 1只 0只 东健 2只 0只 1只 小彬 0只 0只 2只 猎物 税金 兔子 20韩元 鹿 50韩元 野猪 100韩元 运动器械和自行车的价格与需要购买的数量之间的关系可以用两个二维数据表格来表示 2把数字或字母在括号内排列成二维数据表格就叫作矩阵。矩阵的各个数字或字母叫作元素。横向叫作行，纵向叫作列。 矩阵的行数为m，列数为n时，矩阵的大小为m*n，行数和列数数目相等的矩阵叫作方块矩阵。 元素都为0的矩阵叫作零矩阵；主对角线元素都为1，其余元素都是0的矩阵叫作单位矩阵。 3把方程组变成矩阵来解，可以进行一下运算： 某一行可以乘以一个常数：相当于一个方程的两边同时乘以一个常数。 某一行乘以一个常数后可以与另一行相加：相当于一个方程的两边同时乘以一个常数后再与另一个方程相加。 行与行可以对调：方程的顺序对调后，解不变。 4方程组$$\begin{cases} ax+by=p \\ cx+dy=q\end{cases}$$用矩阵表达式如下$$\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}\begin{pmatrix} x \\ y\end{pmatrix}=\begin{pmatrix} p \\ q\end{pmatrix}$$可以简单表示为$$\begin{pmatrix} a &amp; b &amp;: &amp;p \\ c &amp; d &amp;: &amp;q\end{pmatrix}$$ 矩阵的某一行乘以一个常数后形成的新矩阵，求出的解是同一个方程组的解。 矩阵的某一行乘以一个常数后与另一行相加形成的新矩阵，求出的解是同一个方程组的解。 方程组构成的矩阵，行与行可以对调，对调后方程组的解不变。 5可以将下面的三元一次方程组变成矩阵来解$$\begin{cases} a_{11}x+a_{12}y+a_{13}z=p \\ a_{21}x+a_{22}y+a_{23}z=q \\ a_{31}x+a_{32}y+a_{33}z=r\end{cases}$$ $$\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp;: &amp;p \\ a_{21} &amp; a_{22} &amp; a_{23} &amp;: &amp;q \\ a_{31} &amp; a_{32} &amp; a_{33} &amp;: &amp;r\end{pmatrix}$$ 6两个矩阵相等是指两个矩阵中所有的元素对应相等。 两个矩阵之间对应的元素相加，可以实现两个矩阵之间的加法运算。 一个矩阵的各元素可以乘以某个常数。 两个矩阵之间对应的元素相减，可以实现两个矩阵之间的减法运算。 7两个矩阵相乘，前面矩阵的列数要与后面矩阵的行数相等。这时乘积的大小由前面矩阵的行数和后面矩阵的列数决定。即$$A_{m×n}B_{n×t}=C_{m×t}$$ 前面矩阵的第i行和后面矩阵的第j列的各对应元素的乘积之和，构成乘积矩阵中的(i, j)元素。 寻找日常生活中可以用矩阵乘积表示的问题。 8可求出方块矩阵$$\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}$$的逆矩阵 $${\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}}^{-1}= {1 \over ad - bc}\begin{pmatrix} d &amp; -b \\ -c &amp; a\end{pmatrix}(ad-bc \ne 0)$$ 利用逆矩阵求二元一次方程组$$\begin{cases} ax+by=p \\ cx+dy=q\end{cases}$$的解。首先像下面这样用矩阵表示方程：$$\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}\begin{pmatrix} x \\ y\end{pmatrix}=\begin{pmatrix} p \\ q\end{pmatrix}$$然后式子两边靠左的位置写上$$\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}$$的逆矩阵，再做乘法运算。$$\begin{pmatrix} x \\ y\end{pmatrix}={\begin{pmatrix} a &amp; b \\ c &amp; d\end{pmatrix}}^{-1}\begin{pmatrix} p \\ q\end{pmatrix}$$-- $$\begin{pmatrix} x \\ y\end{pmatrix}={1 \over ad - bc}\begin{pmatrix} d &amp; -b \\ -c &amp; a\end{pmatrix}\begin{pmatrix} p \\ q\end{pmatrix}={1 \over ad - bc}\begin{pmatrix} dp-bq \\ -cp+aq\end{pmatrix}(ad-bc \ne 0)$$-- $$x={dp-bq \over ad-bc}, y={-cp+aq \over ad-bc}$$ 9研究是否可以用矩阵表示身边的实际问题。思考如何确定一个矩阵的各个元素。 利用矩阵解决问题，预测未来 各列元素的和等于1的矩阵用俄国数学家马尔可夫的名字命名为马尔可夫转移矩阵。这个矩阵经常用来表示某种变化的概率，某个地区的人口变化、市场经济的版图变化等，我们周围的各种变化都可以用这样的矩阵表示，帮助我们有效地解决问题。 老鼠房间概率城市邻接矩阵]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器]]></title>
    <url>%2F2019%2F03%2F12%2F%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[布隆过滤器什么是布隆过滤器布隆过滤器是一种基于概率进行验重的数据结构。它的基本原理是：小概率事件不容易同时发生。 布隆过滤器使用多个哈希函数把同一个字符串转换成多个不同的哈希值，并记录这些哈希值的特征。下次再面对一个字符串时，布隆过滤器再次使用这些哈希函数把这个字符串转换为多个哈希值。如果这个哈希值全部符合原先那个字符串对应的各个哈希值的特征，则认为这两个字符串是相同的。 哈希函数哈希算法不是一种加密算法，而是一种不可逆的摘要算法。不同的哈希函数可实现不同的哈希算法。使用同一个哈希算法，能够把同一个字符串转成同一个哈希值。 1234import hashlibcode = 'hello'result = hashlib.sha256(code.encode()).hexdigest()print(result) 结果是一个十六进制的_数_ 布隆过滤器的原理 假设K个哈希函数，对同一个字符串计算哈希值，得到K个完全不同的哈希值。 让这K个哈希值同时除以一个数M，就可以得到K个余数。 对于一个新的字符串，重复这个过程，如果新字符串获得的K个余数与原来的字符串对应的K个余数完全相同，那么就可以说，这两个字符串”很有可能”是同一个字符串。 $$1-(1-e^{\frac{-KN}{M}})^{K}$$ 如何压缩数据容量采用二进制保存数字 布隆过滤器与Redis结合使用Redis字符串的位操作，记录K个余数的位置即可。 布隆过滤器的弊端布隆过滤器只能单向验证重复。随着Redis字符串对应的二进制位越来越多的为被设置为1，布隆过滤器误报的概率越来越大，因为可能其它多个字符串对应的二进制位中越来越多的位被设置为1，其中K个值刚好和一个新来的字符串的K个余数重合。提前规划好数据规模与容忍的误报率。 最多需要对 n 个字符串进行验证重复操作，能够容忍的最大误报率为 p，那么，布隆过滤器将会使用到的二进制位的数量为： $$m = -\frac{n\ln{p}}{\ln{2}^{2}}$$ 哈希函数的个数为： $$k = \frac{m}{n}\ln{2}$$]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 Spring Boot 编写 RESTful API]]></title>
    <url>%2F2018%2F12%2F23%2F%E7%94%A8SpringBoot%E7%BC%96%E5%86%99RESTfulAPI%2F</url>
    <content type="text"><![CDATA[课程课程 开始一个最简单的RESTFul API项目RESTful APIRepresentational State Transfer 所有的东西都是资源，所有的操作都是通过对资源的增删改查（CRUD）实现 对资源的增删改查对应对URL的操作（POST,DELETE,PUT,GET） 无状态的 Spring Boot大量使用注解，减少配置，无需配置XML自带嵌入式web服务器 Mavenpom.xml文件是Maven项目的配置文件几个常用的Maven命令（在pom.xml同级目录下运行）mvn test 编译并运行测试用例mvn spring-boot:run 运行spring-boot项目mvn package 打包项目mvn clean 可以和其它命令一起使用，例如mvn clean package 日期型转JSON格式可以在属性上增加1@JsonFormat(timezone=&quot;GMT+8&quot;, pattern=&quot;yyyy-MM-dd&quot;) 或 1@JsonFormat（shape=JsonFormat.Shape.NUMBER） 全局修改可以在application.yml 123456spring: jackson: date-format: yyyy-MM-dd #如果使用字符串表示，用这行设置格式 timezone: GMT+8 serialization: write-dates-as-timestamps: true #使用数值timestamp表示日期 RestController详解热部署（Hot Swapping）pom中加入devtools 12345678&lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 记录日志Commons-logging or SLF4j 12private static final Log log = LogFactory.getLog(Xxx.class);private static final Logger log = LoggerFactory.getLogger(Xxx.class); 日志级别：TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATALapplication.yml配置日志12345logging: file: target/app.log level: ROOT: WARN cn.devmgr: TRACE 命令行工具CURL测试工具postman 在RestController中获取各种相关信息的方法 内容 获取方法 URL中路径的一部分 首先需要在RequestMapping做映射，之后在方法中可以通过注解使用映射的变量。可以写多个，@PutMapping(“/{id}/characters/{cId}”)，还可以使用正则表达式限制类型（不符合要求会返回4xx的错误信息，表示请求参数有问题）此例子表示id必须是数字：@PutMapping(“/{id:\\d+}”) POST方法传递过来的JSON 给参数前增加@RequestBody注解，Spring会自动把POST的Request Body部分的JSON转换成方法声明的类。如果转化失败会返回4xx错误，表示请求参数有问题。 POST方法传递的是表单数据 首先声明传入的是application/x-wwww-form-urlencoded的格式，可在RequestMapping增加consumes参数，@PostMapping(value=”/tvseries”, consumes=MediaType.APPLICATION_FORM_URLENCODED_VALUE)，在方法上增加参数，参数使用@RequestParam注解即可，public Object insert(@RequestParam(value=”name”, required=false) String name)，凡是可以通过HttpServletRequest.getParameter(String)方法取到的值，含表单提交的、QueryString附带的，都可以使用@RequestParam注解得到。默认是request=true。 QueryString的参数 使用@RequestParam注解，通过参数获得,例如：public Object query(@RequestParam(value=”page”, required=false) Integer page) Request Header 可以使用@Requestheader注解获取Request的头信息,例如public ResultJSON editCompany(@RequestHeader(“user-agent”) String userAgent) 注意：@RequestHeader后面的头名字不区分大小写，但RequestParam, PathVariable等是区分的。如果RequestHeader后面不写参数，会用后面的变量名替代。 获取cookie值 使用cookieValue注解，和其它类似，除非为了兼容老现有客户端，新API里不建议用cookie。 获取当前的Request Response 直接写参数，例如：public Object doSomething(HttpServletRequest request, HttpServletResponse response) 获取当前用户 直接在方法上增加参数，类型为org.springframework.security.core.Authentication, 例如：public TvSeries deleteOne(Authentication auth)，参数auth内会存储有当前的用户信息。 文件上传 首先要设置consumes为multipart/form-data，@PostMapping(value=”/files”, consumes=MediaType.MULTIPART_FORM_DATA_VALUE),在方法中写参数 public Map&lt;String, Object&gt; uploadFile(@RequestParam(“file”) MultipartFile file)。在方法中可以直接使用MultipartFile中的流保存文件了。 对客户端传入数据的校验原则：不要相信前端传过来的数据；尽量要前端少传递数据 Bean Validation: JSR303, Hibernate Validator Bean Validation 注解：@Null@NotNull@Min@Max@Size@Past 验证Date@Future@AssertTrue 验证Boolean@AssertFalse@Valid 级联验证注解 类型 注解 任何类型 NULL, NotNull 布尔型 AssertTrue, AssertFalse 字符串 NotBlank, Pattern, Size, Email, DecimalMin, Digits 数值 DecimalMax, DecimalMin, Digits, Max, Min, NegativeOrZero, Positive, PositiveOrZero 集合、Map、List NotEmpty, Size 日期 Future, Past, FutureOrPresent, PastOrPresent 以上注解都指Bean Validation 2.0 定义的注解，在javax.validation.constraints包下。Hibernate有些非JSR标准注解和上面的同名但package不同，功能会和上面这些有些细微差异。 约束规则对子类依旧有效groups参数 每个约束用注解都有一个groups参数 可接收多个class类型（必须是接口） 不声明groups参数是默认组javax.validation.groups.Default 声明了groups参数的会从Default组移除，如需加入Default组需要显示声明，例如@Null(groups={Default.class, Step1.class}) @Valid vs @Validated @Valid是JSR标准定义的注解，只验证Default组的约束 @Validated是Spring定义的注解，可以通过参数来指定验证的组，例如：@Validation({Step1.class,Default.class})表示验证Step1和Default两个组的约束 @Valid可用在成员变量上，进行级联验证；@Validated只能用在参数上，表示这个参数需要验证 参数中只用@Validated，通不过校验的参数，不会执行这个方法，加上BindingResult result ，参数通不过校验也会进入方法执行，校验结果会通过result参数传递进来。 手动验证 12345// 装载验证器@Autowired Validator validator;// 验证某个类，下面是执行默认的验证组，如果需要指定验证组，多传一个class参数Set&lt;ConstraintViolation&lt;?&gt;&gt; result = validator.validate(obj);// 通不过校验result的集合会有值，可以通过size()判断 在Spring Boot项目中使用Mybatis程序的层次结构Web前端、App、小程序、其它系统等Web控制层：@RestController @Controller业务逻辑层：@Service数据访问层：@Repository@Component数据库 PBF: Package by Feature 按功能划分PBL: Package by Layer 按层次划分 相邻层次的数据传输 PO：Persistant Object 持久对象 DTO：Data Transfer Object 数据传输对象 VO：Value Object 或 View Object POJO：Pure Old Java Object 或 Plain Ordinary Java Object DO：Domain Object BO：Business Object 处理业务逻辑 DAO：Data Access Object JavaBean: 有一个public的无参构造方法 属性private，且可以通过get、set、is（可以替代get，用在布尔型属性上）方法或遵循特定命名规范的其它方法访问 可序列化，实现Serializable接口 POJO vs JavaBean: POJO比javabean更简单。POJO严格的遵守简单对象的概念，而一些JavaBean中往往会封装一些简单逻辑。 POJO主要用于数据的临时传递，它只能装载数据，作为数据存储的载体，不具有业务逻辑处理能力。 JavaBean虽然数据的获取与POJO一样，但是javabean当中可以有其他方法。 几种简化方案： 一种POJO从web控制层到数据访问层 用JavaBean代替POJO POJO的get,set写起来也麻烦，用public的field代替 添加Mybatis支持步骤 修改pom.xml，添加mybatis支持 修改application.yml添加数据库连接 修改启动类，增加@MappingScan(“package-of-mapping”)注解 添加Mybatis Mapping接口 添加Mapping对应的XML（可选） pom.xml中添加 123456789&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt;&lt;/dependency&gt; application.yml中添加 1234567spring: datasource: dbcp2.validation-query: select 1 driverClassName: org.postgresql.Driver url: jdbc:postgresql://127.0.0.1:5432/thedb?stringtype=unspecified username: password: Spring Boot 项目的单元测试Assert-JUnit的断言 判断某条件是否为真 Assert.assertTrue(条件表达式)； 判断某条件是否为假 Assert.assertFalse(条件表达式)； 判断两个变量值是否相同 Assert.assertEquals(val1, val2); 判断两个变量值是否不相同 Assert.assertNotEquals(val1, val2); 判断两个数组是否相同 Assert.assertArrayEquals(数组1, 数组2)； 直接测试失败 Assert.fail() Assert.fail(message) Assert vs assert Assert是JUnit的断言类，全名是org.junit.Assert Assert提供了很多静态方法，例如… assert是java关键字，使用方法有两种，表达式为false时，jvm会退出； assert关键字内表达式是否被检查成立依赖jvm的参数，默认是关闭的 Java命令行参数：-ea (enableassertions) -da (disableassertions 默认) 概念 驱动模块 被测模块 桩模块 使用场景：替代尚未开发完毕的子模块；替代对环境依赖较大的子模块（例如数据访问层）； mockito12345&lt;dependency&gt; &lt;groupId&gt;org.mockito&lt;/groupId&gt; &lt;artifactId&gt;mockito-core&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; TDD Test Driven Development (测试驱动开发) 先写测试用例，后写实现代码 重构现有代码时特别好用 RDD: Resume Driven Development 在 Spring 中管理数据库事务@Transactional name 当在配置文件中有多个TransactionManager,可以用该属性指定选择哪个事务管理器。 propagation 事务的传播行为，默认值为REQUIRED。 isolation 事务的隔离度，默认采用DEFAULT。 timeout 事务的超时时间，默认值为-1,。如果超过该时间限制但事务还没有完成，则自动回滚事务。 readOnly 指定事务是否为只读事务，默认值为false；为了忽略那些不需要事务的方法，比如读取数据，可以设置readOnly为true。 rollbackFor 指定能够触发事务回滚的异常类型。 noRollbackFor 指定的异常类型，不回滚事务 1.noRollbackFor或子类；2.rollbackFor或子类；3.throws定义的异常或子类；4.其它异常；5.无异常 @Transactional(propagation=xx) propagation.REQUIRED 如果有事务，那么加入事务，没有的话新建一个（默认） propagation.NOT_SUPPORTED 容器不为这个方法开启事务 propagation.REQUIRED_NEW 不管是否存在事务，都创建一个新的事务，原来的挂起，新的执行完毕，继续执行老的事务 propagation.MANDATORY 必须在一个已有的事务中执行，否则抛出异常 propagation.NEVER 必须在一个没有的事务中执行，否则抛出异常（与propagation.MANDATORY相反） propagation.SUPPORTS 如果其它bean调用这个方法，在其它bean中声明事务，那就用事务，如果其它bean没有声明事务那就不用事务 propagation.NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与propagation.REQUIRED类似的操作 @Transactional(isolation=xx) Isolation.READ_UNCOMMITTED 读未提交数据 （脏读、不可重复读、幻读） Isolation.READ_COMMITTED 读已提交数据（不可重复读、幻读） Isolation.REPEATABLE_READ 可重复读 （幻读） Isolation.SERIALIZATION 串行化 Isolation.DEFAULT 使用数据库默认 Isolation vs Lock 两个不同的东西，隔离不是靠锁实现的，是靠对数据的监控实现的。 锁：表加好锁了，除非出现死锁等特殊情况，事务是不会被数据库主动回滚的。 隔离：如果发现数据不符合数据库隔离级别，当前事务会出错并回滚。相比锁被回滚可能性较大，需要程序有出错重试的步骤。 @Transactional注解的timeout参数 timeout事务的超时时间，默认值为-1,。如果超过该时间限制但事务还没有完成，则自动回滚事务。 方法抛出异常，事务被回滚，可能是SQL执行时间过长的异常，也可能是TransactionTimedOutException 从方法执行开始计算，每个SQL执行前检查一次是否超时，方法全部执行完毕后不检查是否超时 Mybatis进阶复杂类的ORMapping和主子表的同时数据插入#{}可转义 ${} 不可转义（可能导致sql注入） 使用TypeHandler处理枚举、数组、JSON等特殊类型EnumTypeHandler vs EnumOrdinalTypeHandler EnumTypeHandler存储的是对应类的名字，可以存储成一个字符串 EnumOrdinalTypeHandler存储的是枚举类型的顺序 ArrayTypeHandler自定义JsonTypeHandler Spring Security安全控制的层级 基于URL的控制 基于方法的控制 程序内 配置Spring Security12345678910111213141516171819@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true, jsr250Enabled = false)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; private final static Log log = LogFactory.getLog(WebSecurityConfig.class); @Override protected void configure(HttpSecurity httpSecurity) throws Exception &#123; if (log.isTraceEnabled()) &#123; log.trace(&quot;configure httpSecurity...&quot;); &#125; //默认的spring-security配置会让所有的请求都必须在已登录的状况下访问；下面这段代码禁止了这种操作。 httpSecurity.csrf().disable() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS).and() .authorizeRequests().anyRequest().permitAll(); &#125;&#125; Spring Security 注解@EnableGlobalMethodSecurity(prePostEnable=true, securedEnable=true, jsr250Enabled=true)prePostEnable=true: @PreAuthorize @PostAuthorize @PreFilter @PostFiltersecuredEnable=true: @Securedjsr250Enable: @RolesRequied (JSR250) @PreAuthorize @PostAuthorize 中常用的表达式 hasRole(‘user’, ‘admin’) hasAnyRole(‘user’, ‘admin’) hasAuthority(‘query’, ‘update’) hasAnyAuthority(‘query’, ‘update’) permitAll denyAll principal, authentication 当前用户 Role vs Authorization ROLE_开头则是role JSR250 RolesAllowed全部要求是role Spring EL中hasRole也要求是role hasAuthority则不用ROLE_开头 Controller内获取当前用户12345public Object doSomething(Authentication auth)&#123; User u = (User) auth.getPrincipal();&#125;或 Authentication auth = SecurityContextHolder.getContext().getAuthentication(); 启用Spring Security1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; OpenId 提供用户追踪方式 无需使用用户名密码登录 协议2.0版提供属性交换功能 目前已经很少使用 OAuth开放授权 目前是OAuth2.0 2.0不兼容1.0 密码无需告诉第三方 为用户提供一个令牌，允许通过令牌访问资源 OAuth 2.0 Grant Types 授权码模式 Authorization Code 简化模式 Implicit 密码模式 Password 客户端模式 Client Credentials Refresh Token JWT JSON Web Token 三部分：Header、Playload、Verify Signature Header：头部信息，声明类型和加密算法 Playload：载荷 Verify Signature：签名，用于验证头部和载荷部分是否被修改过 JWT的加密方式 HMAC 共用一个秘钥 SHA256 公钥私钥分开 生成JWT和验证JWT的jar12345&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 网站：https://github.com/auth0/java-jwt Spring Async, Scheduling &amp; CacheSpring Async 异步执行配置：@EnableAsync (@SpringBootApplication 那里)使用：@Async （方法上）@Async方法返回值： void Future 其他类型一律返回null；遇到int/double/float/boolean基本类型，执行时会抛出异常：AopInvocationException; Spring Scheduling使用： @EnableScheduling 注解启用Scheduling 方法上加@Scheduled注解，方法会按照参数定期执行@Scheduled 参数： cron 值为字符串 zone 设置时区 fixedDelay (单位毫秒)，每次方法执行完毕后，休息固定时间后再次启动 fixedRate (单位毫秒)按照固定频率启动执行-initialDelay (单位毫秒)，和上面三个参数搭配使用，首次执行延时 集群/负载均衡环境 独立出来一个application运行scheduling task 使用：Quartz Scheduler Spring Cache缓存： 利用java程序中的变量（简单；集群环境中多个实例无法同步） 缓存服务器（Memcached，Redis）Spring中通过注解使用缓存 @EnableCaching启用缓存注解 @Cacheable @CacheEvict @CachePut @CacheConfig 使用 Redis 缓存服务POM中加入依赖 1234&lt;dependecy&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 中加入配置 1234567891011spring: cache: redis: time-to-live: 3306 redis: host: localhost port: 6379 pool: max-active: 5 max-idle: 10 max-wait: 10000 Websocket &amp; JMSWebsocket 全双工通道，数据双向传输 浏览器和服务器之间的持久性的连接 比轮询/长轮询大幅节省资源 使用80/443等HTTP端口 ws://example.com/wsapi wss://secure.example.com/ IE10以上浏览器支持 JMS Java Message Service – Java消息服务 在两个应用程序或者分布式服务之间提供异步消息通讯 应用间解耦 企业应用集成中应用较多 消息服务器很多，Apache ActiveMQ是比较常见的一个 JMS消息模式 点对点（P2P）：每个消息有一个生产者一个消费者 发布者/订阅者（Pub/Sub）:每个消息一个生产者、多个消费者 安装ActiveMQ 配置pom文件 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 配置application.yml 123spring: activemq: broker-url: tcp://127.0.0.1:61616 @EnableJms 注解 部署Spring Boot项目Spring Profile 和配置日志记录框架 可以设置不同的配置参数 可以设置不同的bean的装载 Active Profile 同时可以有多个被激活的profile 有active和default两个概念 如果没有设置active，则spring使用设置的default，如果没有声明default，则使用那些无显示指定的作为default 生产环境架构性能调优 前端：数据缓存；加载顺序；显示顺序；预先加载； NGINX: 设置客户端缓存；数据压缩传输；HTTP2.0 应用：优化算法；优化SQL（慢SQL）；避免N+1查询；异步操作；使用缓存；不常修改数据的静态化；集群； 数据库：索引和统计信息；优化表结构；冗余列和计算列；表拆分；分区表；统计慢SQL（提供程序）；SQL优化建议；升级硬件； 以服务程序运行（ubuntu）杂项在 Spring Boot 项目中使用Servlet,Filter,Listener等Servlet 1234@WebServlet(name=&quot;QrcodeServlet&quot;, urlPatterns=&quot;/servlet/qrcode&quot;)public class QrcodeServlet extends HttpServlet implements Serializable&#123; &#125; Filter 1234@WebFilterpublic class LogFilter implements Filter &#123; &#125; @ServletComponentScan 注解@ComponentScan Autowired 的加载规则@Autowired 查找被注解的变量类型，找到所有此类型的构建或此类型子类的构建。 如果一个也没有找到，看required参数，false则用null，true则失败（默认）。 如果仅找到一个，则装载这个构件。 如果找到多个，且只有一个有@Primary注解，使用Primary的。 如果不符合上述条件，失败。 @Autowired @Qualifier 如果属性既有Autowired注解又有Qualifier注解 在构件中查找名字为Qualifier中指定的名字的注解。 在构件上指定名字的方法有两个，@Service(“这里写名字”)，@Qualifier(“这里写名字”)。 API 的版本客户端传递版本信息方式 URL RequestHeader URL 部署，通过修改NGINX配置，不同域名，不同前缀。 修改application.yml中的contextPath，server.servlet.contextPath:/tutorial-v2 修改@RequestMapping中的参数，例如，@RequestMapping(“/v1/tvseries”) 增加request的参数，例如：/tvseries?version=2 Request Header 自定义request header，例如：Version:2 使用Accept Accept:application/vnd.tutorial.v2 + json 自定义 RequestMappingHandlerMapping 自定义ApiVersion注解 org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping 定制Spring REST的错误返回信息Controller中单独处理 把返回值改成ResponseEntity T为要返回的内容 通过ResponseEntity来设置返回的HttpResponse状态码 全局的异常处理 12345678@RestControllerAdvicepublic class ExceptionHandler &#123; @ExceptionHandler(Throwable.class) @ResponseBody ResponseEntity&lt;Object&gt; handleControllerException(Throwable ex, WebRequest request) &#123; //处理异常，并设置给客户端反馈的信息 &#125;&#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[赠药山高僧惟俨二首]]></title>
    <url>%2F2018%2F12%2F23%2F%E8%B5%A0%E8%8D%AF%E5%B1%B1%E9%AB%98%E5%83%A7%E6%83%9F%E4%BF%A8%E4%BA%8C%E9%A6%96%2F</url>
    <content type="text"><![CDATA[练得身形如鹤形，千株松下两函经。我来问道无馀说，云在青霄水在瓶。 选得幽居惬野情，终年无送亦无迎。有时直上孤峰顶，月下披云啸一声。]]></content>
      <categories>
        <category>Tao</category>
      </categories>
      <tags>
        <tag>Tao</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
